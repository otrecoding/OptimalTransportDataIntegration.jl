var documenterSearchIndex = {"docs":
[{"location":"joint_ot_between_bases/#Optimal-transport-of-the-joint-distribution-between-data-sources","page":"Transport between data sources","title":"Optimal transport of the joint distribution between data sources","text":"","category":"section"},{"location":"joint_ot_between_bases/#OptimalTransportDataIntegration.joint_ot_between_bases_jdot-Tuple{Any}","page":"Transport between data sources","title":"OptimalTransportDataIntegration.joint_ot_between_bases_jdot","text":"joint_ot_between_bases_jdot(\n    data;\n    iterations,\n    learning_rate,\n    batchsize,\n    epochs,\n    hidden_layer_size,\n    reg,\n    reg_m1,\n    reg_m2,\n    Ylevels,\n    Zlevels\n)\n\n\nJoint Distribution Optimal Transport (JDOT) with neural network predictors.\n\nIteratively solves separate OT problems for outcome Y and Z, each matching covariate distributions while minimizing prediction error from outcome-specific discriminant classifiers. This block  coordinate descent algorithm jointly optimizes two transport couplings (G1, G2) and two outcome  prediction networks, enabling outcome-specific matching that balances covariate and outcome  alignment.\n\nThe \"JDOT\" acronym stands for Joint Distribution Optimal Transport, reflecting the method's focus on matching both covariate and outcome distributions across bases through independent  transport plans per outcome.\n\nArguments\n\ndata::DataFrame: Input data with columns database (1 for base A, 2 for base B),  X* covariates, Y (outcome for base B), and Z (outcome for base A)\n\nKeyword Arguments\n\niterations::Int: Number of BCD iterations (OT + network training cycles); default: 10\nlearning_rate::Float64: Adam optimizer learning rate for networks; default: 0.01\nbatchsize::Int: Batch size for stochastic gradient descent; default: 512\nepochs::Int: Training epochs per network at each BCD iteration; default: 500\nhidden_layer_size::Int: Number of neurons in hidden layer; default: 10\nreg::Float64: Entropy regularization for OT (0 = exact, larger = relaxed); default: 0.0\nreg_m1::Float64: Marginal relaxation for base A; default: 0.0\nreg_m2::Float64: Marginal relaxation for base B; default: 0.0\nYlevels::AbstractRange: Categorical levels for outcome Y; default: 1:4\nZlevels::AbstractRange: Categorical levels for outcome Z; default: 1:3\n\nReturns\n\nTuple{Vector{Int}, Vector{Int}}: Predicted outcomes (YB, ZA)\nYB: Final predictions for Y in base B (argmax of network outputs)\nZA: Final predictions for Z in base A (argmax of network outputs)\n\nAlgorithm (Block Coordinate Descent)\n\nInitialize two transport couplings G1, G2 and outcome prediction networks\nFor each BCD iteration: a. Solve two OT problems (one per outcome):\nG1 = argmin ⟨G, C1⟩ matching for Y prediction errors\nG2 = argmin ⟨G, C2⟩ matching for Z prediction errors\nb. Transport outcomes using outcome-specific couplings:\nYB = nB·YA·G1 (Y transported for base B)\nZA = nA·ZB·G2' (Z transported for base A)\nc. Train discriminant networks on transported outcomes:\nTrain modelXYA on (XB, YB) for Y prediction\nTrain modelXZB on (XA, ZA) for Z prediction\nd. Compute prediction errors (cross-entropy):\nloss_y = YA vs modelXYA(XB)\nloss_z = ZB vs modelXZB(XA)\ne. Update cost matrices with prediction error feedback:\nC1 ← C0 + loss_y\nC2 ← C0 + loss_z'\nf. Check convergence: transport plan stability or cost stability\nReturn argmax of final network predictions\n\nKey Innovations of JDOT\n\nSeparate couplings per outcome: G1 for Y matching, G2 for Z matching\nOutcome-specific cost: Each outcome has its own cost matrix driven by prediction errors\nJoint optimization: Balances covariate matching and outcome prediction accuracy\nIterative refinement: Feedback loop where prediction errors guide next OT solve\n\nDifferences from Related Methods\n\njointotbetweenbaseswith_predictors: Single transport plan G; symmetric outcome treatment\njointotbetweenbasesjdot (this): Two transport plans G1, G2; outcome-specific matching\ndaoutcomeswith_predictors: Single static OT solve; no cost refinement\njointotbetweenbasescategory: No discriminant networks; pure OT on outcomes\n\nDetails\n\nSeparate OT problems: Each outcome solved independently, capturing outcome-specific distributions\nCost matrix updates: Prediction errors guide next OT solve for better outcome alignment\nCross-entropy loss: Weights outcome prediction errors by 1/n_levels for fair comparison\nTransport scaling: YB and ZA scaled by sample counts for proper probability aggregation\nDiscriminant classifiers: Neural networks predict outcomes from covariates on transported samples\nUnbalanced OT: Uses KL divergence for regularization when reg > 0\n\nSee Also\n\njoint_ot_between_bases_with_predictors: Single coupling variant\njoint_ot_between_bases_da_outcomes_with_predictors: DA variant (single static OT)\njoint_ot_between_bases_category: OT without discriminant networks\nJointOTBetweenBases: Main method dispatcher\n\nNotes\n\nMost sophisticated iterative approach: two independent transport problems per iteration\nComputationally expensive: ~2× cost of single-coupling methods per iteration\nOutcome-specific matching allows capturing different covariate-outcome relationships per outcome\nUseful when Y and Z have fundamentally different relationships to X\nConvergence typically faster than symmetric methods due to outcome-guided matching\n\n\n\n\n\n","category":"method"},{"location":"joint_ot_between_bases/#OptimalTransportDataIntegration.joint_ot_between_bases_with_predictors-Tuple{Any}","page":"Transport between data sources","title":"OptimalTransportDataIntegration.joint_ot_between_bases_with_predictors","text":"joint_ot_between_bases_with_predictors(\n    data;\n    iterations,\n    learning_rate,\n    batchsize,\n    epochs,\n    hidden_layer_size,\n    reg,\n    reg_m1,\n    reg_m2,\n    Ylevels,\n    Zlevels\n)\n\n\nHybrid statistical matching combining optimal transport and neural network predictors.\n\nAlternates between two operations: (1) solving optimal transport problem to match  base A and B samples by covariate-outcome combinations, and (2) training neural networks  to learn outcome prediction functions that minimize cross-entropy loss given transport plan. This block coordinate descent (BCD) algorithm jointly optimizes coupling and predictors.\n\nArguments\n\ndata::DataFrame: Input data with columns database (1 for base A, 2 for base B),  X* covariates, Y (outcome for base B), and Z (outcome for base A)\n\nKeyword Arguments\n\niterations::Int: Number of BCD iterations (OT + network training cycles); default: 10\nlearning_rate::Float64: Adam optimizer learning rate for networks; default: 0.01\nbatchsize::Int: Batch size for stochastic gradient descent; default: 512\nepochs::Int: Training epochs per network at each BCD iteration; default: 500\nhidden_layer_size::Int: Number of neurons in hidden layer; default: 10\nreg::Float64: Entropy regularization for OT (0 = exact, larger = relaxed); default: 0.0\nreg_m1::Float64: Marginal relaxation for base A; default: 0.0\nreg_m2::Float64: Marginal relaxation for base B; default: 0.0\nYlevels::AbstractRange: Categorical levels for outcome Y; default: 1:4\nZlevels::AbstractRange: Categorical levels for outcome Z; default: 1:3\n\nReturns\n\nTuple{Vector{Int}, Vector{Int}}: Predicted outcomes (YB, ZA)\nYB: Final predictions for Y in base B (argmax of network outputs)\nZA: Final predictions for Z in base A (argmax of network outputs)\n\nAlgorithm (Block Coordinate Descent)\n\nInitialize transport plan G, cost matrix C, and weights (uniform)\nConcatenate covariates with outcomes: XYA = [XA; YA], XZB = [XB; ZB]\nFor each BCD iteration:\nSolve OT problem to compute coupling G minimizing C\nUpdate outcome targets: YB = nB·YA·G, ZA = nA·ZB·G'\nTrain predictor networks on updated targets\nCompute cross-entropy loss and update cost matrix C ← C₀ + loss_feedback\nCheck convergence: transport plan stability (delta) or cost stability\nReturn argmax of final network predictions\n\nDetails\n\nTransport plan: Couples samples across bases; G[i,j] represents mass transported from A[i] to B[j]\nPredictors: Neural networks learn mapping (X,outcome) → opposite_outcome with loss feedback\nCost update: Cross-entropy loss between outcomes and network predictions guides next OT iteration\nUnbalanced OT: Uses KL divergence for regularization when reg > 0\n\nSee Also\n\njoint_ot_between_bases_category: OT without neural network predictors\nsimple_learning: Supervised baseline without OT coupling\nJointOTBetweenBases: Main method dispatcher\n\nNotes\n\nComputationally expensive: trains two networks at each of ~10 BCD iterations\nCombines strength of OT (covariate matching) with flexibility of neural networks\nRequires more iterations/epochs than pure OT for convergence\n\n\n\n\n\n","category":"method"},{"location":"joint_ot_between_bases/#OptimalTransportDataIntegration.joint_ot_between_bases_without_outcomes-Tuple{Any}","page":"Transport between data sources","title":"OptimalTransportDataIntegration.joint_ot_between_bases_without_outcomes","text":"joint_ot_between_bases_without_outcomes(\n    data;\n    iterations,\n    learning_rate,\n    batchsize,\n    epochs,\n    hidden_layer_size,\n    reg,\n    reg_m1,\n    reg_m2,\n    Ylevels,\n    Zlevels\n)\n\n\nOptimal transport matching with outcome prediction from covariates only.\n\nSolves OT problem on covariate space with single transport coupling G, then trains neural network predictors to map covariates to outcomes. Networks learn to predict missing outcomes directly from covariates (X → Y, X → Z) without using outcome combinations in the OT matching. This \"without  outcomes\" variant focuses on covariate-outcome mapping rather than joint distribution transport.\n\nArguments\n\ndata::DataFrame: Input data with columns database (1 for base A, 2 for base B),  X* covariates, Y (outcome for base B), and Z (outcome for base A)\n\nKeyword Arguments\n\niterations::Int: Number of BCD iterations (OT + network training cycles); default: 10\nlearning_rate::Float64: Adam optimizer learning rate for networks; default: 0.01\nbatchsize::Int: Batch size for stochastic gradient descent; default: 512\nepochs::Int: Training epochs per network at each BCD iteration; default: 1000\nhidden_layer_size::Int: Number of neurons in hidden layer; default: 10\nreg::Float64: Entropy regularization for OT (0 = exact, larger = relaxed); default: 0.0\nreg_m1::Float64: Marginal relaxation for base A; default: 0.0\nreg_m2::Float64: Marginal relaxation for base B; default: 0.0\nYlevels::AbstractRange: Categorical levels for outcome Y; default: 1:4\nZlevels::AbstractRange: Categorical levels for outcome Z; default: 1:3\n\nReturns\n\nTuple{Vector{Int}, Vector{Int}}: Predicted outcomes (YB, ZA)\nYB: Final predictions for Y in base B (argmax of network outputs)\nZA: Final predictions for Z in base A (argmax of network outputs)\n\nAlgorithm (Block Coordinate Descent)\n\nInitialize single transport plan G and covariate-only predictor networks\nFor each BCD iteration: a. Solve OT problem on covariate space only:\nG = argmin ⟨G, C⟩ (no outcome information in cost)\nb. Transport outcome distributions using single coupling:\nYB = nB·YA·G (Y transported for base B)\nZA = nA·ZB·G' (Z transported for base A)\nc. Train predictor networks from covariates to transported outcomes:\nTrain modelXA on (XA, ZA) - learn X → Z\nTrain modelXB on (XB, YB) - learn X → Y\nd. Compute prediction errors (cross-entropy):\nloss_y = YA vs modelXB(XB)\nloss_z = ZB vs modelXA(XA)\ne. Update cost matrix with prediction error feedback:\nC ← C₀ + lossy + lossz'\nf. Check convergence: transport plan stability or cost stability\nReturn argmax of final network predictions\n\nKey Differences from Similar Methods\n\nwithout_outcomes (this): OT on X only; networks predict X → Y, X → Z\nwith_predictors: OT on X; networks train on transported (X,outcome) pairs\njdot: Two separate OT problems (one per outcome); outcome-specific matching\nda_outcomes: OT on X; direct outcome distribution transport (no networks)\n\nDetails\n\nCovariate-only OT: Cost matrix depends only on X distance, ignoring outcomes\nSymmetric treatment: Both Y and Z use same transport coupling G\nLoss-driven refinement: Prediction errors guide next OT cost matrix\nNetwork inputs: Covariate-only (X), not joint (X,outcome)\nTransport targets: Networks learn to match transported outcome distributions\nUnbalanced OT: Uses KL divergence for regularization when reg > 0\nSingle coupling: Unlike JDOT (2 couplings), uses one G for both outcomes\n\nSee Also\n\njoint_ot_between_bases_with_predictors: Joint (X,outcome) OT with predictors\njoint_ot_between_bases_jdot: Two separate OT problems (outcome-specific)\njoint_ot_between_bases_da_covariables: DA on covariates (no outcome info)\nJointOTBetweenBases: Main method dispatcher\n\nNotes\n\nSimplest BCD variant: matches on covariates, learns outcome relationships via networks\nComputationally efficient: single OT solve per iteration (vs 2 for JDOT)\nOutcome-agnostic matching: OT doesn't see outcome relationships directly\nTwo networks learn identical relationship (X → Y and X → Z) from different transported targets\nUseful when outcome dimensions are complex or when covariate alignment is priority\n\n\n\n\n\n","category":"method"},{"location":"joint_ot_between_bases/#OptimalTransportDataIntegration.joint_ot_between_bases_category-NTuple{4, Any}","page":"Transport between data sources","title":"OptimalTransportDataIntegration.joint_ot_between_bases_category","text":"joint_ot_between_bases_category(\n    data,\n    reg,\n    reg_m1,\n    reg_m2;\n    Ylevels,\n    Zlevels,\n    iterations\n)\n\n\nStatistical matching via optimal transport between two bases with categorical outcomes.\n\nThis function integrates two data sources (base A and base B) using optimal transport theory to match and predict missing categorical outcomes (Y and Z). It transports joint distributions of covariates and outcomes across bases while iteratively minimizing cross-entropy loss for outcome predictions.\n\nArguments\n\ndata::DataFrame: Input data with columns database (1 for base A, 2 for base B),  X* covariates, Y (outcome for base B), and Z (outcome for base A)\nreg::Float64: Entropy regularization parameter for OT solver (0 = exact OT, larger = more relaxed)\nreg_m1::Float64: Marginal constraint relaxation for base A (set to 0 for balanced problem)\nreg_m2::Float64: Marginal constraint relaxation for base B (set to 0 for balanced problem)\n\nKeyword Arguments\n\nYlevels::AbstractRange: Categorical levels for outcome Y; default: 1:4\nZlevels::AbstractRange: Categorical levels for outcome Z; default: 1:3\niterations::Int: Number of algorithm iterations for cost matrix refinement; default: 1\n\nReturns\n\nTuple{Vector{Int32}, Vector{Int32}}: Predicted outcomes (YBpred, ZApred)\nYB_pred: Predictions for Y in base A (same length as base A observations)\nZA_pred: Predictions for Z in base B (same length as base B observations)\n\nAlgorithm\n\nAggregate individuals by covariate and outcome combinations\nInitialize transport plan G and cost matrix C based on Euclidean distance\nIterate:\nSolve OT problem (unbalanced or exact) via PythonOT\nPredict outcomes by minimizing cross-entropy loss per transport cost\nUpdate cost matrix with prediction error feedback\nReturn predicted outcomes mapped back to original base structure\n\nDetails\n\nOne-hot encodes categorical outcomes for cross-entropy loss computation\nUses unbalanced OT (KL divergence) when regm1 > 0 and regm2 > 0, else exact EMD\nConvergence: detects transport plan convergence (delta < 1e-16) or cost stability (< 1e-7)\n\nSee Also\n\njoint_ot_between_bases_discrete: Discrete covariates version\none_hot_encoder: Converts categorical data to one-hot matrix form\n\n\n\n\n\n","category":"method"},{"location":"joint_ot_between_bases/#OptimalTransportDataIntegration.joint_ot_between_bases_da_covariables-Tuple{Any}","page":"Transport between data sources","title":"OptimalTransportDataIntegration.joint_ot_between_bases_da_covariables","text":"joint_ot_between_bases_da_covariables(\n    data;\n    learning_rate,\n    batchsize,\n    epochs,\n    hidden_layer_size,\n    reg,\n    reg_m1,\n    reg_m2,\n    Ylevels,\n    Zlevels\n)\n\n\nOptimal transport matching with discriminant analysis on covariates.\n\nCombines optimal transport on covariate space with neural network discriminant classifiers. Solves OT problem based on covariate-only distance, then trains neural networks to predict outcomes from transported covariates. The \"DA\" (Discriminant Analysis) prefix indicates that prediction uses only covariates (X), not the joint (X,outcome) space like other DA variants.\n\nArguments\n\ndata::DataFrame: Input data with columns database (1 for base A, 2 for base B),  X* covariates, Y (outcome for base B), and Z (outcome for base A)\n\nKeyword Arguments\n\nlearning_rate::Float64: Adam optimizer learning rate for networks; default: 0.01\nbatchsize::Int: Batch size for stochastic gradient descent; default: 512\nepochs::Int: Training epochs per network; default: 500\nhidden_layer_size::Int: Number of neurons in hidden layer; default: 10\nreg::Float64: Entropy regularization for OT (0 = exact, larger = relaxed); default: 0.0\nreg_m1::Float64: Marginal relaxation for base A; default: 0.0\nreg_m2::Float64: Marginal relaxation for base B; default: 0.0\nYlevels::AbstractRange: Categorical levels for outcome Y; default: 1:4\nZlevels::AbstractRange: Categorical levels for outcome Z; default: 1:3\n\nReturns\n\nTuple{Vector{Int}, Vector{Int}}: Predicted outcomes (YB, ZA)\nYB: Final predictions for Y in base B (argmax of network outputs)\nZA: Final predictions for Z in base A (argmax of network outputs)\n\nAlgorithm\n\nInitialize networks for outcome prediction: modelXYA (X → Y), modelXZB (X → Z)\nCompute cost matrix C based on Euclidean distance in covariate space only (not outcomes)\nSolve OT problem: find coupling G that minimizes ⟨G, C⟩\nTransport covariates: XAtransported = nB·XA·G, XBtransported = nA·XB·G'\nTrain networks on transported covariates:\nTrain modelXYA on (XB_transported, YA)\nTrain modelXZB on (XA_transported, ZB)\nReturn final predictions on original covariates\n\nKey Differences from Other DA Variants\n\nda_covariables (this): OT on X only; predict Y,Z from X (covariate-only discriminant analysis)\nda_outcomes: OT on joint (X,Y) and (X,Z); may refine based on outcome prediction errors\ndaoutcomeswith_predictors: Similar to da_outcomes but with explicit predictor networks\n\nDetails\n\nCost matrix: Based purely on covariate distances (SqEuclidean)\nNetwork training: Uses transported covariates as targets to match covariate distributions\nTransport plan: Couples full samples (not disaggregated by outcome), only balances marginal covariate distributions\nUnbalanced OT: Uses KL divergence for regularization when reg > 0\n\nSee Also\n\njoint_ot_between_bases_da_outcomes: OT on outcomes with discriminant prediction\njoint_ot_between_bases_da_outcomes_with_predictors: Hybrid with explicit predictor networks\njoint_ot_between_bases_category: OT without discriminant analysis\njoint_ot_between_bases_with_predictors: OT with flexible predictor networks\n\nNotes\n\nComputationally less expensive than outcome-based DA variants\nMay lose information by ignoring outcome distributions in OT matching\nNetwork training uses transported covariate targets, not standard supervised learning\nUseful when covariate matching is priority and outcome prediction is secondary\n\n\n\n\n\n","category":"method"},{"location":"joint_ot_between_bases/#OptimalTransportDataIntegration.joint_ot_between_bases_da_outcomes-Tuple{Any}","page":"Transport between data sources","title":"OptimalTransportDataIntegration.joint_ot_between_bases_da_outcomes","text":"joint_ot_between_bases_da_outcomes(\n    data;\n    iterations,\n    learning_rate,\n    batchsize,\n    epochs,\n    hidden_layer_size,\n    reg,\n    reg_m1,\n    reg_m2,\n    Ylevels,\n    Zlevels\n)\n\n\nDiscriminant analysis via optimal transport on covariate-outcome space.\n\nSolves a single optimal transport problem to match distributions across bases, then uses the resulting transport coupling to predict outcomes. Unlike iterative refinement methods, this approach computes outcomes directly from transported outcome distributions without predictor networks. The \"DA outcomes\" variant focuses on transporting outcome probability distributions aligned with covariate matching.\n\nArguments\n\ndata::DataFrame: Input data with columns database (1 for base A, 2 for base B),  X* covariates, Y (outcome for base B), and Z (outcome for base A)\n\nKeyword Arguments\n\niterations::Int: Unused parameter (kept for API compatibility); default: 10\nlearning_rate::Float64: Unused parameter (kept for API compatibility); default: 0.01\nbatchsize::Int: Unused parameter (kept for API compatibility); default: 512\nepochs::Int: Unused parameter (kept for API compatibility); default: 500\nhidden_layer_size::Int: Unused parameter (kept for API compatibility); default: 10\nreg::Float64: Entropy regularization for OT (0 = exact, larger = relaxed); default: 0.0\nreg_m1::Float64: Marginal relaxation for base A; default: 0.0\nreg_m2::Float64: Marginal relaxation for base B; default: 0.0\nYlevels::AbstractRange: Categorical levels for outcome Y; default: 1:4\nZlevels::AbstractRange: Categorical levels for outcome Z; default: 1:3\n\nReturns\n\nTuple{Vector{Int}, Vector{Int}}: Predicted outcomes (YB, ZA)\nYB: Final predictions for Y in base B (argmax of transported outcome probabilities)\nZA: Final predictions for Z in base A (argmax of transported outcome probabilities)\n\nAlgorithm\n\nCompute cost matrix C based on Euclidean distance between covariates (X only)\nInitialize uniform weights: wa = 1/nA, wb = 1/nB\nSolve OT problem once: G = argmin ⟨G, C⟩ subject to marginal constraints\nUses unbalanced OT (KL divergence) if reg > 0, else exact EMD\nTransport outcome distributions using coupling G:\nYBpred = softmax(nB·YA·G): transported Y probabilities for base B samples\nZApred = softmax(nA·ZB·G'): transported Z probabilities for base A samples\nReturn argmax of transported probabilities as final predictions\n\nKey Differences from Other DA Variants\n\nda_covariables: OT on X only; predicts outcomes from transported covariates via networks\nda_outcomes (this): OT on X; predicts outcomes directly from transported outcome distributions\ndaoutcomeswith_predictors: OT on X; trains networks on transported outcome targets\njointotbetweenbaseswith_predictors: Iterative BCD with loss-driven cost updates\n\nDetails\n\nCost matrix: Based purely on covariate distances (SqEuclidean), not normalized\nSingle OT solve: Static cost matrix, solved once (no iterative refinement)\nOutcome transport: Coupling G transports outcome probability distributions across bases\nDirect prediction: Predictions come directly from transported probabilities (softmax + argmax)\nNo networks: Unlike with_predictors variant, uses no neural networks\nUnbalanced OT: Uses KL divergence for regularization when reg > 0\nScaling factor: Transportation weighted by sample count (nB, nA) for proper probability aggregation\n\nSee Also\n\njoint_ot_between_bases_da_outcomes_with_predictors: DA outcomes with explicit predictor networks\njoint_ot_between_bases_da_covariables: DA on covariates with covariate-based prediction\njoint_ot_between_bases_category: OT without discriminant analysis\njoint_ot_between_bases_with_predictors: Iterative BCD variant with cost refinement\n\nNotes\n\nSimplest DA variant: combines OT matching with direct outcome transport\nMost computationally efficient: single OT solve, no network training\nOutcome predictions are soft probabilities from transported distributions\nUseful for understanding how outcome distributions align under OT matching\nNo learnable parameters; purely distribution-based prediction\nHyperparameters for networks (learning_rate, etc.) are ignored for API compatibility\n\n\n\n\n\n","category":"method"},{"location":"joint_ot_between_bases/#OptimalTransportDataIntegration.joint_ot_between_bases_da_outcomes_with_predictors-Tuple{Any}","page":"Transport between data sources","title":"OptimalTransportDataIntegration.joint_ot_between_bases_da_outcomes_with_predictors","text":"joint_ot_between_bases_da_outcomes_with_predictors(\n    data;\n    iterations,\n    learning_rate,\n    batchsize,\n    epochs,\n    hidden_layer_size,\n    reg,\n    reg_m1,\n    reg_m2,\n    Ylevels,\n    Zlevels\n)\n\n\nAdvanced discriminant analysis: OT on covariate space with outcome-based predictor refinement.\n\nSolves a single optimal transport problem on covariate-only distance, then trains neural networks to predict outcomes from transported samples. This \"DA with predictors\" variant uses the OT coupling to transport outcome distributions and trains networks on transported outcome targets (not iterative). Combines covariate-based matching with outcome prediction flexibility.\n\nArguments\n\ndata::DataFrame: Input data with columns database (1 for base A, 2 for base B),  X* covariates, Y (outcome for base B), and Z (outcome for base A)\n\nKeyword Arguments\n\niterations::Int: Unused parameter (kept for API compatibility); default: 10\nlearning_rate::Float64: Adam optimizer learning rate for networks; default: 0.01\nbatchsize::Int: Batch size for stochastic gradient descent; default: 512\nepochs::Int: Training epochs per network; default: 500\nhidden_layer_size::Int: Number of neurons in hidden layer; default: 10\nreg::Float64: Entropy regularization for OT (0 = exact, larger = relaxed); default: 0.0\nreg_m1::Float64: Marginal relaxation for base A; default: 0.0\nreg_m2::Float64: Marginal relaxation for base B; default: 0.0\nYlevels::AbstractRange: Categorical levels for outcome Y; default: 1:4\nZlevels::AbstractRange: Categorical levels for outcome Z; default: 1:3\n\nReturns\n\nTuple{Vector{Int}, Vector{Int}}: Predicted outcomes (YB, ZA)\nYB: Final predictions for Y in base B (argmax of network outputs)\nZA: Final predictions for Z in base A (argmax of network outputs)\n\nAlgorithm\n\nInitialize outcome prediction networks: modelXYA (X → Y), modelXZB (X → Z)\nCompute cost matrix C based on Euclidean distance in covariate space only\nSolve OT problem once: G = argmin ⟨G, C⟩ subject to marginal constraints\nTransport outcome distributions (not covariates):\nYBt = nB·YA·G (transported Y probabilities for base B samples)\nZAt = nA·ZB·G' (transported Z probabilities for base A samples)\nTrain networks on transported outcome targets:\nTrain modelXYA on (XB, YBt) - predict transported Y from base B covariates\nTrain modelXZB on (XA, ZAt) - predict transported Z from base A covariates\nReturn final predictions on original covariates\n\nKey Differences from Other DA Variants\n\nda_covariables: OT on X only; networks train on transported covariates (not outcomes)\nda_outcomes: OT on joint (X,Y) and (X,Z); iterative refinement (not implemented here)\ndaoutcomeswith_predictors (this): OT on X only; networks train on transported outcomes\njointotbetweenbaseswith_predictors: Iterative BCD with loss-driven cost updates\n\nDetails\n\nCost matrix: Based purely on covariate distances (SqEuclidean), normalized\nSingle OT solve: Unlike BCD methods, OT problem solved once with static cost\nOutcome transport: Coupling G transports outcome probability distributions across bases\nNetwork targets: Soft probabilities (one-hot encoded) from transported distributions\nUnbalanced OT: Uses KL divergence for regularization when reg > 0\nPrediction: Uses softmax probabilities, then argmax for final discrete predictions\n\nSee Also\n\njoint_ot_between_bases_da_outcomes: Similar approach (outcomes) without explicit predictors\njoint_ot_between_bases_da_covariables: OT on covariates with covariate-based DA\njoint_ot_between_bases_with_predictors: Iterative BCD variant with cost refinement\njoint_ot_between_bases_category: OT without discriminant analysis component\n\nNotes\n\nMore computationally efficient than iterative BCD methods (single OT solve)\nBlends OT matching (for covariate alignment) with neural network flexibility (outcome prediction)\nNetwork training targets are transported outcome distributions, not standard supervised learning\nOutcome transport via coupling ensures covariate-aligned outcome distributions\nUseful when outcome prediction needs flexibility beyond linear transport assumptions\n\n\n\n\n\n","category":"method"},{"location":"joint_ot_between_bases/#OptimalTransportDataIntegration.joint_ot_between_bases_discrete-NTuple{4, Any}","page":"Transport between data sources","title":"OptimalTransportDataIntegration.joint_ot_between_bases_discrete","text":"joint_ot_between_bases_discrete(\n    data,\n    reg,\n    reg_m1,\n    reg_m2;\n    Ylevels,\n    Zlevels,\n    iterations,\n    distance\n)\n\n\nStatistical matching via optimal transport for discrete (categorical) covariates.\n\nSpecialized implementation for discrete covariate data. Aggregates individuals by unique covariate-outcome combinations to reduce computational burden. Uses one-hot encoded covariates and solves OT problems on the aggregated space. Iteratively minimizes cross-entropy loss between predicted and transported outcomes to guide cost matrix refinement.\n\nArguments\n\ndata::DataFrame: Input data with columns database (1 for base A, 2 for base B),  X* covariates (must be categorical/integer), Y (outcome for base B), and Z (outcome for base A)\nreg::Float64: Entropy regularization parameter for OT solver\nreg_m1::Float64: Marginal constraint relaxation for base A\nreg_m2::Float64: Marginal constraint relaxation for base B\n\nKeyword Arguments\n\nYlevels::AbstractRange: Categorical levels for outcome Y; default: 1:4\nZlevels::AbstractRange: Categorical levels for outcome Z; default: 1:3\niterations::Int: Number of algorithm iterations; default: 1\ndistance::Distances.Metric: Distance metric for covariate space; default: Euclidean()\n\nReturns\n\nTuple{Vector{Int}, Vector{Int}}: Predicted outcomes (YB, ZA)\nYB: Predictions for Y in base B\nZA: Predictions for Z in base A\n\nAlgorithm\n\nOne-hot encode discrete covariates\nAggregate individuals by unique (X, outcome) combinations:\nReduces individuals to \"cells\" (covariate × outcome modality combinations)\nComputes cell weights as proportions of total sample size\nFilters out empty cells\nCompute pairwise distances between covariate profiles in aggregated space\nInitialize outcome predictions and loss matrices\nFor each iteration:\nSolve OT problem on aggregated cells with current cost matrix\nPredict outcomes by minimizing cross-entropy loss (argmin over modalities)\nCompute prediction error (cross-entropy between transported and predicted outcomes)\nUpdate cost matrix with loss feedback\nCheck convergence\nMap predictions back to original individual-level data\nReturn individual-level outcome predictions\n\nComputational Efficiency Tricks\n\nAggregation: Reduces from n individuals to ~(ncovariates × noutcomes) cells\nE.g., 1000 individuals with 3 covariates (2,3,4 levels) + 2 outcomes → ~200-300 cells\nOne-hot encoding: Discrete covariates represented as binary vectors for distance computation\nFiltered weights: Only processes cells with positive weight (observed data combinations)\nInstance pre-computation: Distance matrix computed once between all covariate profiles\n\nDetails\n\nData aggregation: Crucial for computational tractability with discrete data\nDistance metric: Determines covariate similarity (Euclidean on one-hot vectors by default)\nCost matrix: Initial distance-based, updated with loss feedback\nCross-entropy: Compares one-hot encoded outcomes for alignment\nConvergence: Checks transport plan stability (delta) and cost stability\n\nSee Also\n\njoint_ot_between_bases_category: Continuous covariate version with categorical outcomes\nInstance: Pre-computed distance structure used in aggregation\n\nNotes\n\nDiscrete specialization: Exploits discrete structure via aggregation for efficiency\nCell-level matching: Matches entire cells (all individuals with same X and outcome)\nCross-entropy loss: Drives iterative cost refinement for outcome prediction accuracy\nOutcome aggregation: Predictions map back from cell-level to individual-level deterministically\n\n\n\n\n\n","category":"method"},{"location":"learning/#Machine-learning-wihout-optimal-transport","page":"Machine Learning","title":"Machine learning wihout optimal transport","text":"","category":"section"},{"location":"learning/#OptimalTransportDataIntegration.onehot-Tuple{AbstractMatrix}","page":"Machine Learning","title":"OptimalTransportDataIntegration.onehot","text":"onehot(x::AbstractMatrix)\n\nConvert categorical covariates to one-hot encoded matrix format.\n\nFor each column in input matrix, creates binary vectors for each level (1:4), where col .== level produces a binary vector. Stacks all one-hot vectors into rows of output matrix for use in neural network input.\n\nArguments\n\nx::AbstractMatrix: Categorical data matrix (nsamples, ncovariates)\n\nReturns\n\nMatrix{Float32}: One-hot encoded matrix (nsamples * nlevels, ncovariates) where nlevels = 4 (hardcoded for discrete covariates)\n\n\n\n\n\n","category":"method"},{"location":"learning/#OptimalTransportDataIntegration.simple_learning-Tuple{Any}","page":"Machine Learning","title":"OptimalTransportDataIntegration.simple_learning","text":"simple_learning(\n    data;\n    hidden_layer_size,\n    learning_rate,\n    batchsize,\n    epochs,\n    Ylevels,\n    Zlevels\n)\n\n\nStatistical matching baseline via supervised neural networks.\n\nTrains two independent 2-layer neural networks: one predicting Y (outcome for base B)  from X in base A, and one predicting Z (outcome for base A) from X in base B.  This reference method ignores joint distribution structure, serving as a comparison  point to validate optimal transport improvements.\n\nArguments\n\ndata::DataFrame: Input data with columns database (1 for base A, 2 for base B),  X* covariates, Y (outcome for base B), and Z (outcome for base A)\n\nKeyword Arguments\n\nhidden_layer_size::Int: Number of neurons in hidden layer; default: 10\nlearning_rate::Float64: Adam optimizer learning rate; default: 0.01\nbatchsize::Int: Batch size for stochastic gradient descent; default: 512\nepochs::Int: Training epochs for each network; default: 1000\nYlevels::AbstractRange: Categorical levels for outcome Y; default: 1:4\nZlevels::AbstractRange: Categorical levels for outcome Z; default: 1:3\n\nReturns\n\nTuple{Vector{Int}, Vector{Int}}: Predicted outcomes (YB, ZA)\nYB: Predictions for Y in base B (argmax of network outputs)\nZA: Predictions for Z in base A (argmax of network outputs)\n\nModel Architecture\n\nTwo identical networks:\n\ninput (one-hot encoded X) → Dense(hidden_layer_size) → Dense(n_levels) → softmax → argmax\n\nAlgorithm\n\nSplit data by database indicator (base A, base B)\nOne-hot encode covariates X for each base\nOne-hot encode outcomes Y and Z with specified levels\nTrain modelA on (XA, YA) and modelB on (XB, ZB) with logit cross-entropy loss\nPredict YB = argmax(modelA(XB)) and ZA = argmax(modelB(XA))\n\nSee Also\n\nJointOTBetweenBases: OT-based method that captures joint distributions\nJointOTWithinBase: Within-source distribution balancing\n\nNotes\n\nIndependent networks ignore covariate-outcome correlation structure across bases\nServes as a performance baseline for validating OT methods\nUses Flux.jl for neural network training with Adam optimizer\n\n\n\n\n\n","category":"method"},{"location":"learning/#OptimalTransportDataIntegration.learning_with_continuous_data-Tuple{Any}","page":"Machine Learning","title":"OptimalTransportDataIntegration.learning_with_continuous_data","text":"learning_with_continuous_data(\n    data;\n    hidden_layer_size,\n    learning_rate,\n    batchsize,\n    epochs\n)\n\n\nStatistical matching baseline via supervised neural networks on continuous covariates.\n\nTrains two independent 2-layer neural networks: one predicting Y (outcome for base B)  from continuous X in base A, and one predicting Z (outcome for base A) from continuous X  in base B. This reference method ignores the joint distribution structure, serving as a  comparison point to validate optimal transport improvements for continuous data.\n\nArguments\n\ndata::DataFrame: Input data with columns database (1 for base A, 2 for base B),  X* continuous covariates, Y (outcome for base B), and Z (outcome for base A)\n\nKeyword Arguments\n\nhidden_layer_size::Int: Number of neurons in hidden layer; default: 10\nlearning_rate::Float64: Adam optimizer learning rate; default: 0.01\nbatchsize::Int: Batch size for stochastic gradient descent; default: 128\nepochs::Int: Training epochs for each network; default: 1000\n\nReturns\n\nTuple{Vector{Int}, Vector{Int}}: Predicted outcomes (YB, ZA)\nYB: Predictions for Y in base B (argmax of network outputs, categories 1:4)\nZA: Predictions for Z in base A (argmax of network outputs, categories 1:3)\n\nModel Architecture\n\nTwo identical networks with continuous input:\n\ninput (continuous X) → Dense(input_dim → hidden_layer_size) → Dense(hidden_layer_size → n_levels) → softmax → argmax\n\nAlgorithm\n\nSplit data by database indicator (base A, base B)\nExtract continuous covariates X (no one-hot encoding needed)\nOne-hot encode outcomes Y and Z with specified levels (Y: 1:4, Z: 1:3)\nTrain modelA on (XA, YA) and modelB on (XB, ZB) with logit cross-entropy loss\nPredict YB = argmax(modelA(XB)) and ZA = argmax(modelB(XA))\n\nDifferences from simple_learning (discrete version)\n\nInput: continuous covariates (no one-hot encoding)\nNetwork input dimension: number of continuous features (not one-hot expanded)\nUses transposed covariate matrix for Flux compatibility\nOtherwise identical supervised learning approach\n\nSee Also\n\nsimple_learning: Discrete covariates version with one-hot encoded X\nJointOTBetweenBases: OT-based method that captures joint distributions\njoint_ot_between_bases_with_predictors: Hybrid OT + neural network approach\n\nNotes\n\nIndependent networks ignore covariate-outcome correlation structure across bases\nServes as a performance baseline for validating OT methods on continuous data\nUses Flux.jl for neural network training with Adam optimizer\nMore computationally efficient than discrete version due to smaller input dimension\n\n\n\n\n\n","category":"method"},{"location":"#OptimalTransportDataIntegration.jl","page":"Quickstart","title":"OptimalTransportDataIntegration.jl","text":"","category":"section"},{"location":"#Helper-functions","page":"Quickstart","title":"Helper functions","text":"","category":"section"},{"location":"#OptimalTransportDataIntegration.DataParameters","page":"Quickstart","title":"OptimalTransportDataIntegration.DataParameters","text":"struct DataParameters\n\nScenario parameters for synthetic data generation.\n\nSpecifies all hyperparameters controlling data generation for both discrete and continuous covariate settings. Includes sample sizes, covariate distributions, effect sizes, noise levels, and outcome category probabilities. Can be persisted to/from JSON for reproducible experiments.\n\nFields\n\nnA::Int: Sample size for base A; default: 1000\nnB::Int: Sample size for base B; default: 1000\nmA::Vector{Float64}: Mean vector for base A covariates (MVN); default: [0.0, 0.0, 0.0]\nmB::Vector{Float64}: Mean vector for base B covariates (MVN); default: [1.0, 1.0, 0.0]\ncovA::Matrix{Float64}: Covariance matrix for base A covariates; default: 3×3 with diag=1, off-diag=0.2\ncovB::Matrix{Float64}: Covariance matrix for base B covariates; default: 3×3 with diag=1, off-diag=0.2\naA::Vector{Float64}: Effect sizes (regression coefficients) for base A outcomes; default: [1.0, 1.0, 1.0]\naB::Vector{Float64}: Effect sizes (regression coefficients) for base B outcomes; default: [1.0, 1.0, 1.0]\nr2::Float64: Coefficient of determination (signal-to-noise ratio); default: 0.6 (higher = stronger signal)\npA::Vector{Vector{Float64}}: Category probabilities for base A categorical covariates (one vector per dimension)\ndefault: [[0.5, 0.5], [1/3, 1/3, 1/3], [0.25, 0.25, 0.25, 0.25]]\nFor discrete generator: multinomial probabilities\nFor continuous generator: unused\npB::Vector{Vector{Float64}}: Category probabilities for base B categorical covariates (one vector per dimension)\ndefault: [[0.8, 0.2], [1/3, 1/3, 1/3], [0.25, 0.25, 0.25, 0.25]]\nFor discrete generator: multinomial probabilities\nFor continuous generator: unused\n\nUsage\n\nCreate with defaults:\n\nparams = DataParameters()\n\nCustomize selected fields:\n\nparams = DataParameters(nA=500, nB=500, r2=0.8, mB=[2.0, 2.0, 0.0])\n\nKey Concepts\n\nCovariate Shift: Controlled by different mA/mB (continuous) or pA/pB (discrete)\nSample Imbalance: Controlled by different nA/nB\nSignal Strength: Controlled by r2 (0 = pure noise, 1 = perfect signal)\nEffect Sizes: aA/aB control how strongly outcomes depend on covariates\nBases Design: Base A has outcome Z, Base B has outcome Y; X covariates overlap\n\nSee Also\n\nDiscreteDataGenerator: Generator for discrete covariate scenarios\nContinuousDataGenerator: Generator for continuous covariate scenarios\nread: Load parameters from JSON file\nsave: Persist parameters to JSON file\n\nNotes\n\nDimensions: length(mA) = size(covA,1) and length(aA) = length(mA) must hold\nProbabilities in pA/pB must sum to 1.0 for each dimension\nr2 controls noise via σ²_error = (1/r² - 1) × Var(X'a)\n\n\n\n\n\n","category":"type"},{"location":"#OptimalTransportDataIntegration.read-Tuple{AbstractString}","page":"Quickstart","title":"OptimalTransportDataIntegration.read","text":"read(jsonfile)\n\n\nLoad data generation scenario parameters from a JSON file.\n\nDeserializes a JSON file containing all DataParameters fields and reconstructs a DataParameters instance. Useful for reproducible experiments across sessions and for tracking scenario configurations.\n\nArguments\n\njsonfile::AbstractString: Path to JSON file containing scenario parameters\n\nReturns\n\nDataParameters: Reconstructed parameter object with all fields populated\n\nFile Format\n\nJSON structure with top-level keys matching DataParameters field names:\n\n{\n  \"nA\": 1000,\n  \"nB\": 1000,\n  \"mA\": [0.0, 0.0, 0.0],\n  \"mB\": [1.0, 1.0, 0.0],\n  \"covA\": [[1.0, 0.2, 0.2], [0.2, 1.0, 0.2], [0.2, 0.2, 1.0]],\n  \"covB\": [[1.0, 0.2, 0.2], [0.2, 1.0, 0.2], [0.2, 0.2, 1.0]],\n  \"aA\": [1.0, 1.0, 1.0],\n  \"aB\": [1.0, 1.0, 1.0],\n  \"r2\": 0.6,\n  \"pA\": [[0.5, 0.5], [0.333, 0.333, 0.333], [0.25, 0.25, 0.25, 0.25]],\n  \"pB\": [[0.8, 0.2], [0.333, 0.333, 0.333], [0.25, 0.25, 0.25, 0.25]]\n}\n\nExample\n\nparams = read(\"scenario_1.json\")\ngen = DiscreteDataGenerator(params)\ndata = generate(gen)\n\nSee Also\n\nsave: Persist parameters to JSON\nDataParameters: Parameter type\n\n\n\n\n\n","category":"method"},{"location":"#OptimalTransportDataIntegration.save-Tuple{AbstractString, DataParameters}","page":"Quickstart","title":"OptimalTransportDataIntegration.save","text":"save(jsonfile, params)\n\n\nPersist data generation scenario parameters to a JSON file.\n\nSerializes all DataParameters fields to a JSON file for later retrieval. Essential for reproducible experiments: save parameters used in generation, then load them later to recreate identical scenarios.\n\nArguments\n\njsonfile::AbstractString: Path where JSON file will be written\nparams::DataParameters: Parameter object to serialize\n\nReturns\n\nNothing; writes JSON file as side effect\n\nExample\n\nparams = DataParameters(nA=500, nB=500, r2=0.8)\nsave(\"my_scenario.json\", params)\n\n# Later...\nparams_loaded = read(\"my_scenario.json\")\n@assert params == params_loaded  # Exact reproduction\n\nSee Also\n\nread: Load parameters from JSON\nDataParameters: Parameter type\n\n\n\n\n\n","category":"method"},{"location":"#OptimalTransportDataIntegration.ContinuousDataGenerator","page":"Quickstart","title":"OptimalTransportDataIntegration.ContinuousDataGenerator","text":"struct ContinuousDataGenerator\n\nFactory for generating synthetic datasets with continuous covariates and categorical outcomes.\n\nGenerates two data sources (base A and B) with multivariate normal covariates (X) and  categorical outcomes (Y, Z) derived from linear combinations of X plus noise. The generator  pre-computes outcome quantiles and binning thresholds during construction for efficient  repeated generation via generate().\n\nFields\n\nparams::DataParameters: Scenario parameters (means, covariances, effect sizes, outcome probabilities)\nbinsYA::Vector{Float64}: Quantile thresholds for digitizing outcome Y in base A\nbinsZA::Vector{Float64}: Quantile thresholds for digitizing outcome Z in base A\nbinsYB::Vector{Float64}: Quantile thresholds for digitizing outcome Y in base B\nbinsZB::Vector{Float64}: Quantile thresholds for digitizing outcome Z in base B\n\nConstructor\n\nContinuousDataGenerator(params; scenario=1, n=10000)\n\nArguments\n\nparams::DataParameters: Data scenario parameters including:\nnA, nB: Sample sizes for bases A and B\nmA, mB: Mean vectors for multivariate normal covariates\ncovA, covB: Covariance matrices for covariates\naA, aB: Effect size vectors (regression coefficients on X)\nr2: Coefficient of determination (controls noise level)\npA, pB: Outcome category probabilities (unused for continuous generation)\n\nKeyword Arguments\n\nscenario::Int: Determines binning strategy; default: 1\nscenario=1: Same binning thresholds for A and B (no covariate shift)\nscenario≠1: Separate binning thresholds per base (covariate shift)\nn::Int: Unused parameter (kept for API compatibility); default: 10000\n\nGeneration Process (Constructor)\n\nSample covariates: XA ~ N(mA, covA) and XB ~ N(mB, covB)\nCompute error variance from r² and effect sizes: σ²_error = (1/r² - 1) × ∑(aᵢ aⱼ Cov[i,j])\nGenerate latent outcomes: Y = X'a + ε with ε ~ N(0, σ²_error)\nCompute quantiles at [0.25, 0.5, 0.75] for Y and [1/3, 2/3] for Z\nCreate bins: - q₁ q₂ q₃  (4 categories) and - q₁ q₂  (3 categories)\n\nSee Also\n\nDiscreteDataGenerator: For categorical covariates\ngenerate: Call this method on generator to produce DataFrame\n\nNotes\n\nCovariates are continuous (multivariate normal)\nOutcomes are always categorical (3-4 classes based on quantile binning)\nR² parameter controls signal-to-noise ratio in linear model\nScenario parameter allows testing covariate shift and distribution mismatch assumptions\n\n\n\n\n\n","category":"type"},{"location":"#OptimalTransportDataIntegration.generate-Tuple{ContinuousDataGenerator}","page":"Quickstart","title":"OptimalTransportDataIntegration.generate","text":"generate(generator; eps)\n\n\nGenerate a synthetic dataset with continuous covariates and categorical outcomes.\n\nSamples new covariates from the multivariate normal distributions specified in generator, computes latent outcomes via linear model, then discretizes into categorical bins pre-computed during generator construction.\n\nArguments\n\ngenerator::ContinuousDataGenerator: Pre-configured generator with binning thresholds\n\nKeyword Arguments\n\neps::Float64: Offset added to outcome Z binning thresholds (for covariate shift sensitivity); default: 0.0\n\nReturns\n\nDataFrame: Dataset with columns:\nX1, X2, ...: Continuous covariates (dimension matches params.mA length)\nY: Categorical outcome (1:4) indicating base B's observed outcome\nZ: Categorical outcome (1:3) indicating base A's observed outcome\ndatabase: Integer (1 for base A, 2 for base B) indicating data source\nTotal rows: params.nA + params.nB\n\nAlgorithm\n\nSample covariates: XA ~ N(mA, covA) and XB ~ N(mB, covB)\nCompute latent outcomes: Y1 = XA'·aA + εA and Y2 = XB'·aB + εB\nDigitize into categories using pre-computed bins:\nYA = digitize(Y1, binsYA), ZA = digitize(Y1, binsZA)\nYB = digitize(Y2, binsYB + eps), ZB = digitize(Y2, binsZB + eps)\nAssemble DataFrame with database indicator\nLog category distributions (info level)\n\nDetails\n\nData split: Base A (database=1) has outcomes Z, Base B (database=2) has outcomes Y\nMissing outcomes: Implicit in database indicator (no NAs in columns)\nOutcome relationship: Z and Y in same base are derived from same latent variable\neps parameter: Small perturbation useful for testing robustness to binning threshold changes\n\nExample\n\nparams = DataParameters(nA=500, nB=500, r2=0.6)\ngen = ContinuousDataGenerator(params)\ndata = generate(gen)  # 1000 rows × 5 columns\n\nSee Also\n\nContinuousDataGenerator: Constructor with binning logic\nDiscreteDataGenerator / generate: Discrete covariates alternative\n\n\n\n\n\n","category":"method"},{"location":"#OptimalTransportDataIntegration.DiscreteDataGenerator","page":"Quickstart","title":"OptimalTransportDataIntegration.DiscreteDataGenerator","text":"struct DiscreteDataGenerator\n\nFactory for generating synthetic datasets with discrete (categorical) covariates and categorical outcomes.\n\nGenerates two data sources (base A and B) with categorical covariates (X) sampled from  multinomial distributions and categorical outcomes (Y, Z) derived from linear combinations of  X indicators plus noise. The generator pre-computes outcome quantiles and binning thresholds  during construction for efficient repeated generation via generate().\n\nFields\n\nparams::DataParameters: Scenario parameters (outcome probabilities, effect sizes, r²)\ncovA::Matrix{Float64}: Empirical covariance matrix of covariates in base A\ncovB::Matrix{Float64}: Empirical covariance matrix of covariates in base B\nbinsYA::Vector{Float64}: Quantile thresholds for digitizing outcome Y in base A\nbinsZA::Vector{Float64}: Quantile thresholds for digitizing outcome Z in base A\nbinsYB::Vector{Float64}: Quantile thresholds for digitizing outcome Y in base B\nbinsZB::Vector{Float64}: Quantile thresholds for digitizing outcome Z in base B\n\nConstructor\n\nDiscreteDataGenerator(params; scenario=1, n=10000)\n\nArguments\n\nparams::DataParameters: Data scenario parameters including:\nnA, nB: Sample sizes for bases A and B\npA, pB: Outcome category probabilities as vectors of vectors\npA[1]: Probabilities for 1st covariate categories\npA[2]: Probabilities for 2nd covariate categories, etc.\naA, aB: Effect size vectors (regression coefficients for covariate indicators)\nr2: Coefficient of determination (controls noise level)\n\nKeyword Arguments\n\nscenario::Int: Determines binning strategy; default: 1\nscenario=1: Same binning thresholds for A and B (no covariate shift)\nscenario≠1: Separate binning thresholds per base (covariate shift)\nn::Int: Unused parameter (kept for API compatibility); default: 10000\n\nGeneration Process (Constructor)\n\nSample categorical covariates: XA[i,j] ~ Categorical(pA[j]) for each dimension\nCompute empirical covariances of sampled covariates\nCompute error variance from r² and effect sizes: σ²_error = (1/r² - 1) × ∑(aᵢ aⱼ Cov[i,j])\nGenerate latent outcomes: Y = X'a + ε with ε ~ N(0, σ²_error) (when σ² > 0)\nCompute quantiles at [0.25, 0.5, 0.75] for Y and [1/3, 2/3] for Z\nCreate bins: - q₁ q₂ q₃  (4 categories) and - q₁ q₂  (3 categories)\n\nSee Also\n\nContinuousDataGenerator: For continuous (multivariate normal) covariates\ngenerate: Call this method on generator to produce DataFrame\n\nNotes\n\nCovariates are categorical (sampled from multinomial distributions)\nOutcomes are always categorical (3-4 classes based on quantile binning)\nR² parameter controls signal-to-noise ratio in linear model\nScenario parameter allows testing covariate shift and distribution mismatch assumptions\nCovariance matrices computed from sampled data may vary between generator instances\n\n\n\n\n\n","category":"type"},{"location":"#OptimalTransportDataIntegration.generate-Tuple{DiscreteDataGenerator}","page":"Quickstart","title":"OptimalTransportDataIntegration.generate","text":"generate(generator; eps)\n\n\nGenerate a synthetic dataset with discrete (categorical) covariates and categorical outcomes.\n\nSamples new categorical covariates from multinomial distributions specified in generator, computes latent outcomes via linear model on covariate indicators, then discretizes into  categorical bins pre-computed during generator construction.\n\nArguments\n\ngenerator::DiscreteDataGenerator: Pre-configured generator with binning thresholds and covariance estimates\n\nKeyword Arguments\n\neps::Float64: Offset added to outcome Z binning thresholds (for covariate shift sensitivity); default: 0.0\n\nReturns\n\nDataFrame: Dataset with columns:\nX1, X2, ...: Categorical covariates (dimension matches length(params.pA))\nY: Categorical outcome (1:4) indicating base B's observed outcome\nZ: Categorical outcome (1:3) indicating base A's observed outcome\ndatabase: Integer (1 for base A, 2 for base B) indicating data source\nTotal rows: params.nA + params.nB\n\nAlgorithm\n\nSample categorical covariates: XA[i,j] ~ Categorical(pA[j]) for each base independently\nCompute latent outcomes: Y1 = XA'·aA + εA and Y2 = XB'·aB + εB\nDigitize into categories using pre-computed bins:\nYA = digitize(Y1, binsYA), ZA = digitize(Y1, binsZA)\nYB = digitize(Y2, binsYB + eps), ZB = digitize(Y2, binsZB + eps)\nAssemble DataFrame with database indicator\nLog category distributions (info level)\n\nDetails\n\nData split: Base A (database=1) has outcomes Z, Base B (database=2) has outcomes Y\nMissing outcomes: Implicit in database indicator (no NAs in columns)\nOutcome relationship: Z and Y in same base are derived from same latent variable\neps parameter: Small perturbation useful for testing robustness to binning threshold changes\nCovariate sampling: Independent samples each call, so distributions may vary slightly from params\n\nExample\n\nparams = DataParameters(nA=500, nB=500, r2=0.6)\ngen = DiscreteDataGenerator(params, scenario=1)\ndata = generate(gen)  # 1000 rows × 5 columns (3 covariates + Y + Z + database)\n\nSee Also\n\nDiscreteDataGenerator: Constructor with binning logic\nContinuousDataGenerator / generate: Continuous covariates alternative\n\n\n\n\n\n","category":"method"},{"location":"#OptimalTransportDataIntegration.accuracy","page":"Quickstart","title":"OptimalTransportDataIntegration.accuracy","text":"accuracy(ypred, ytrue)\n\n\n\n\n\n\naccuracy(data, yb_pred, za_pred)\n\n\n\n\n\n\naccuracy(sol)\n\n\n\n\n\n\n","category":"function"},{"location":"#OptimalTransportDataIntegration.confusion_matrix","page":"Quickstart","title":"OptimalTransportDataIntegration.confusion_matrix","text":"confusion_matrix(y_true, y_pred; classes)\n\n\nmade by claude.ai\n\n\n\n\n\n","category":"function"},{"location":"#OptimalTransportDataIntegration.compute_pred_error!","page":"Quickstart","title":"OptimalTransportDataIntegration.compute_pred_error!","text":"compute_pred_error!(sol, inst)\ncompute_pred_error!(sol, inst, proba_disp)\ncompute_pred_error!(sol, inst, proba_disp, mis_disp)\ncompute_pred_error!(\n    sol,\n    inst,\n    proba_disp,\n    mis_disp,\n    full_disp\n)\n\n\nCompute prediction errors in a solution\n\n\n\n\n\n","category":"function"},{"location":"joint_ot_within_base/#Optimal-transport-of-the-joint-distribution-within-a-data-source","page":"Transport within a data source","title":"Optimal transport of the joint distribution within a data source","text":"","category":"section"},{"location":"joint_ot_within_base/#OptimalTransportDataIntegration.Instance","page":"Transport within a data source","title":"OptimalTransportDataIntegration.Instance","text":"struct Instance\n\nDefinition and initialization of an Instance structure\n\ndatafile : file name\ndistance : ∈ ( Cityblock, Euclidean, Hamming )\nindXA    : indexes of subjects of A with given X value\nindXB    : indexes of subjects of B with given X value\n\n\n\n\n\n","category":"type"},{"location":"joint_ot_within_base/#OptimalTransportDataIntegration.joint_ot_within_base_continuous-Tuple{Any}","page":"Transport within a data source","title":"OptimalTransportDataIntegration.joint_ot_within_base_continuous","text":"joint_ot_within_base_continuous(\n    data;\n    lambda,\n    alpha,\n    percent_closest,\n    distance,\n    Ylevels,\n    Zlevels\n)\n\n\nBalance within-base distributions via optimal transport for continuous covariates.\n\nSpecialized within-base method for continuous covariate data. Solves linear programming  problem to transport and reweight individuals within each base (A and B) independently, balancing covariate and outcome distributions. Does NOT match across bases—solves two  separate OT problems (one per base) to improve within-base alignment of outcomes to  marginal covariate distributions.\n\nArguments\n\ndata::DataFrame: Input data with columns database (1 for base A, 2 for base B),  X* continuous covariates, Y (outcome for base B), and Z (outcome for base A)\n\nKeyword Arguments\n\nlambda::Float64: Regularization weight for covariate smoothness; default: 0.392\nalpha::Float64: Outcome alignment weight; default: 0.714\npercent_closest::Float64: Fraction of closest neighbors for cost averaging; default: 0.2\ndistance::Distances.Metric: Distance metric for covariate space; default: Euclidean()\nYlevels::AbstractRange: Categorical levels for outcome Y; default: 1:4\nZlevels::AbstractRange: Categorical levels for outcome Z; default: 1:3\n\nReturns\n\nTuple{Vector{Int}, Vector{Int}}: Predicted outcomes (YB, ZA)\nYB: Balanced outcome predictions for base B\nZA: Balanced outcome predictions for base A\n\nAlgorithm\n\nDigitize continuous covariates into categorical bins using quartiles\nCompute median covariate values per bin for distance computation\nBuild distance matrix D between individuals (base A vs base B)\nAggregate individuals by covariate-outcome combinations\nCompute cost matrix C[y,z] by averaging distances between:\nY=y individuals and closest Z=z individuals\nZ=z individuals and closest Y=y individuals\nBuild linear program for each base (A, B) independently:\nDecision variables: joint probabilities γ[x,y,z] for (covariate, Y, Z)\nConstraints: marginal consistency with observed distributions\nObjective: minimize transportation cost with covariate and outcome regularization\nSolve LP using Clp solver\nExtract and return predicted outcomes\n\nKey Design Choices\n\nWithin-base: Solves independent LP for base A and base B (no cross-base matching)\nContinuous specialization: Digitizes continuous covariates; uses median bin values\nRegularization: lambda controls smoothness of covariate reweighting\nNeighborhood-based cost: Averages distances to percent_closest neighbors (robust)\nLinear programming: Exact solution via Clp (not approximate like unbalanced OT)\n\nDetails\n\nCovariate binning: Uses quantiles [0.25, 0.5, 0.75] to create 4 bins\nDistance computation: Euclidean on median bin values within each base\nCost matrix: Symmetric averaging (YZ + ZY) prevents asymmetric bias\nMarginal constraints: Maintains observed covariate and outcome distributions\nOutcome prediction: Extracted from optimal coupling as argmax probability\n\nSee Also\n\njoint_ot_within_base_discrete: Discrete covariate version with aggregation\nJointOTWithinBase: Main dispatcher for within-base methods\nJointOTBetweenBases: Between-bases matching (different approach)\n\nNotes\n\nReference implementation: Demonstrates balanced LP approach vs OT methods\nWithin-base only: Does not integrate across bases (limited integration benefit)\nContinuous focus: Leverages continuous structure via median binning\nNo cross-base info: Cannot use cross-base information for matching\nModerate scalability: LP solver may be slower than OT for large n\nExact solution: Linear program guarantees optimality (vs approximate methods)\n\n\n\n\n\n","category":"method"},{"location":"joint_ot_within_base/#OptimalTransportDataIntegration.ot_joint-Tuple{Instance, Float64, Float64, Float64}","page":"Transport within a data source","title":"OptimalTransportDataIntegration.ot_joint","text":"ot_joint(\n    inst,\n    alpha,\n    lambda,\n    percent_closest;\n    norme,\n    aggregate_tol,\n    verbose\n)\n\n\nSolve linear program for within-base outcome balancing via optimal transport.\n\nCore solver for within-base OT matching on discrete covariates. Takes pre-computed Instance with distance matrices and individual aggregation, then formulates and solves linear programs (one per base A, B) to find joint distributions of (X, Y, Z) that balance covariate and  outcome distributions while minimizing transportation cost.\n\nArguments\n\ninst::Instance: Pre-computed instance with distance matrices and aggregation indices\nalpha::Float64: Weight balancing covariate regularization (higher = prioritize covariates)\nlambda::Float64: Coefficient controlling overall regularization strength\npercent_closest::Float64: Fraction of closest neighbors for cost computation (robustness)\n\nKeyword Arguments\n\nnorme::Metric: Distance metric for covariate neighbors; default: Euclidean()\naggregate_tol::Float64: Tolerance for covariate aggregation (unused in main algorithm); default: 0.5\nverbose::Bool: Enable detailed logging of results; default: false\n\nReturns\n\nSolution: Object containing optimal transport plan and outcome predictions\n\nAlgorithm Details\n\nPre-aggregate individuals by similar covariate values (using Instance)\nCompute cost matrix C[y,z] from instance pre-computed distances\nBuild linear program per base with:\nVariables: γ[x,y,z] joint probabilities of (covariate, Y, Z)\nConstraints: marginals match observed distributions\nObjective: minimize transport cost + regularization\nSolve LP using Clp (simplex algorithm)\nExtract solution and compute outcome predictions\n\nModel Structure\n\nThe LP formulates the problem:\n\nmin  ⟨C, γ⟩ + λ·(regularization on covariate matching)\ns.t. marginal constraints ensuring statistical consistency\n     γ ≥ 0 (probabilities non-negative)\n\nOptimization Targets\n\nCovariate alignment: Regularization ensures covariate marginals stay close to observed\nOutcome prediction: Cost matrix drives outcome rebalancing via OT\nStability: Neighborhood averaging and regularization prevent overfitting\n\nSee Also\n\nInstance: Pre-computation structure\naverage_distance_to_closest: Cost matrix computation\nSolution: Output structure\n\nNotes\n\nRequires pre-computed Instance for efficiency\nTwo independent LP solves (base A and base B)\nUses Clp solver for exactness (not approximate)\n\n\n\n\n\n","category":"method"},{"location":"joint_ot_within_base/#OptimalTransportDataIntegration.average_distance_to_closest-Tuple{Instance, Float64}","page":"Transport within a data source","title":"OptimalTransportDataIntegration.average_distance_to_closest","text":"average_distance_to_closest(inst, percent_closest)\n\n\nCompute the cost between pairs of outcomes as the average distance between covariations of individuals with these outcomes, but considering only the percent closest neighbors\n\n\n\n\n\n","category":"method"},{"location":"joint_ot_within_base/#OptimalTransportDataIntegration.Solution","page":"Transport within a data source","title":"OptimalTransportDataIntegration.Solution","text":"mutable struct Solution\n\ntsolve       : solution time\njointYZA     : joint distribution of Y and Z in A\njointYZB     : joint distribution of Y and Z in B\nestimatorZA  : estimator of probability of Z for individuals in base A\nestimatorYB  : estimator of probability of Y for individuals in base B\n\n\n\n\n\n","category":"type"},{"location":"simulations/#Evaluation-of-the-methods","page":"Numerical experiments","title":"Evaluation of the methods","text":"JDOT-wi  Transport of the joint distribution of explanatory variables and outcomes within a data source (widehatmathcalP_1n and widehatmathcalP_1n). The parameter of  relaxation is chosen alpha=0, and regularization lambda=0 \nJDOT-wi-r  Regularized transport of the joint distribution of explanatory variables and outcomes within a data source (widehatmathcalP_1n and widehatmathcalP_2n). We added a relaxation on the constraints and a regularization term. The parameter of  relaxation is chosen alpha=04, and regularization lambda=07 \nJDOT-be Balanced transport of explanatory variables and estimated outcomes between data sources (widehatmathcalP_2n).  We used 10 iterations for the BCD algorithm. The parameters are chosen alpha_1=1max(mathcalL_1), and alpha_2=1max(mathcalL_2). \nJDOT-be-r-un Regularized unbalanced transport of explanatory variables and estimated outcomes between data sources (widehatmathcalP_2n). For classifiers f and g, we used nearest neighbor method with one neighbor.  We used 10 iterations for the BCD algorithm. We added an entropic regularization.  The parameter of regularization is chosen  lambda = 001 and the parameter of relaxation is m = 001.\n\nTo evaluate the performance of the methods, we compute the  accuracy of prediction of Z in A and Y in B. The impact of the elements characterizing the simulations will be studied through the following scenarios.","category":"section"},{"location":"simulations/#Effect-of-the-sample-size.","page":"Numerical experiments","title":"Effect of the sample size.","text":"Keeping p, m^A, m^B and a, we investigate the impact of the n choosing ninleft100100010000right.","category":"section"},{"location":"simulations/#Effect-of-the-ratio-of-data-sources-sizes.","page":"Numerical experiments","title":"Effect of the ratio of data sources sizes.","text":"Database A and B have different sample size n^A and n^B respectively.  Keeping m^A, m^B and a and n_A=1000 we investigate the impact of the n^An^B choosing n^An^Binleft1210right.","category":"section"},{"location":"simulations/#Effect-of-the-link-between-the-covariates-and-Y-and-Z.","page":"Numerical experiments","title":"Effect of the link between the covariates and Y and Z.","text":"Keeping n, m^A, m^B and a, we investigate the impact of the p choosing pinleft02040608right.","category":"section"},{"location":"simulations/#Covariate-shift-assumption","page":"Numerical experiments","title":"Covariate shift assumption","text":"Keeping p and a, we investigate the impact of differences in the distributions of X^A and X^B by considering the following four scenarios: \n\nm^A=(000), m^B=(000), \nm^A=(000), m^B=(100),\nm^A=(000), m^B=(110),\nm^A=(000), m^B=(120).","category":"section"},{"location":"simulations/#Changes-in-Conditional-distribution-YX-and-ZX.","page":"Numerical experiments","title":"Changes in Conditional distribution YX and ZX.","text":"Finally, we wish to evaluate the importance of satisfying the assumption that the distributions of Y and Z given X are the same in the two databases. For this, we replace the quartile t^Z and tertile t^Y by t^Z+ epsilon and t^Y+ epsilon in database B. Keeping p, m^A and m^B=(000), a^A= (111111), we consider the following four scenarios: epsilon = (0 01 05 1).","category":"section"}]
}
