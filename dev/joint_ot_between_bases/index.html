<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Transport between data sources · OptimalTransportDataIntegration</title><meta name="title" content="Transport between data sources · OptimalTransportDataIntegration"/><meta property="og:title" content="Transport between data sources · OptimalTransportDataIntegration"/><meta property="twitter:title" content="Transport between data sources · OptimalTransportDataIntegration"/><meta name="description" content="Documentation for OptimalTransportDataIntegration."/><meta property="og:description" content="Documentation for OptimalTransportDataIntegration."/><meta property="twitter:description" content="Documentation for OptimalTransportDataIntegration."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">OptimalTransportDataIntegration</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Quickstart</a></li><li><a class="tocitem" href="../joint_ot_within_base/">Transport within a data source</a></li><li class="is-active"><a class="tocitem" href>Transport between data sources</a></li><li><a class="tocitem" href="../learning/">Machine Learning</a></li><li><a class="tocitem" href="../simulations/">Numerical experiments</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Transport between data sources</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Transport between data sources</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/otrecoding/OptimalTransportDataIntegration.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/otrecoding/OptimalTransportDataIntegration.jl/blob/main/docs/src/joint_ot_between_bases.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Optimal-transport-of-the-joint-distribution-between-data-sources"><a class="docs-heading-anchor" href="#Optimal-transport-of-the-joint-distribution-between-data-sources">Optimal transport of the joint distribution between data sources</a><a id="Optimal-transport-of-the-joint-distribution-between-data-sources-1"></a><a class="docs-heading-anchor-permalink" href="#Optimal-transport-of-the-joint-distribution-between-data-sources" title="Permalink"></a></h1><article><details class="docstring" open="true"><summary id="OptimalTransportDataIntegration.joint_ot_between_bases_jdot-Tuple{Any}"><a class="docstring-binding" href="#OptimalTransportDataIntegration.joint_ot_between_bases_jdot-Tuple{Any}"><code>OptimalTransportDataIntegration.joint_ot_between_bases_jdot</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">joint_ot_between_bases_jdot(
    data;
    iterations,
    learning_rate,
    batchsize,
    epochs,
    hidden_layer_size,
    reg,
    reg_m1,
    reg_m2,
    Ylevels,
    Zlevels
)
</code></pre><p>Joint Distribution Optimal Transport (JDOT) with neural network predictors.</p><p>Iteratively solves separate OT problems for outcome Y and Z, each matching covariate distributions while minimizing prediction error from outcome-specific discriminant classifiers. This block  coordinate descent algorithm jointly optimizes two transport couplings (G1, G2) and two outcome  prediction networks, enabling outcome-specific matching that balances covariate and outcome  alignment.</p><p>The &quot;JDOT&quot; acronym stands for <strong>Joint Distribution Optimal Transport</strong>, reflecting the method&#39;s focus on matching both covariate and outcome distributions across bases through independent  transport plans per outcome.</p><p><strong>Arguments</strong></p><ul><li><code>data::DataFrame</code>: Input data with columns <code>database</code> (1 for base A, 2 for base B),  <code>X*</code> covariates, <code>Y</code> (outcome for base B), and <code>Z</code> (outcome for base A)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>iterations::Int</code>: Number of BCD iterations (OT + network training cycles); default: 10</li><li><code>learning_rate::Float64</code>: Adam optimizer learning rate for networks; default: 0.01</li><li><code>batchsize::Int</code>: Batch size for stochastic gradient descent; default: 512</li><li><code>epochs::Int</code>: Training epochs per network at each BCD iteration; default: 500</li><li><code>hidden_layer_size::Int</code>: Number of neurons in hidden layer; default: 10</li><li><code>reg::Float64</code>: Entropy regularization for OT (0 = exact, larger = relaxed); default: 0.0</li><li><code>reg_m1::Float64</code>: Marginal relaxation for base A; default: 0.0</li><li><code>reg_m2::Float64</code>: Marginal relaxation for base B; default: 0.0</li><li><code>Ylevels::AbstractRange</code>: Categorical levels for outcome Y; default: 1:4</li><li><code>Zlevels::AbstractRange</code>: Categorical levels for outcome Z; default: 1:3</li></ul><p><strong>Returns</strong></p><ul><li><code>Tuple{Vector{Int}, Vector{Int}}</code>: Predicted outcomes (YB, ZA)<ul><li><code>YB</code>: Final predictions for Y in base B (argmax of network outputs)</li><li><code>ZA</code>: Final predictions for Z in base A (argmax of network outputs)</li></ul></li></ul><p><strong>Algorithm (Block Coordinate Descent)</strong></p><ol><li>Initialize two transport couplings G1, G2 and outcome prediction networks</li><li>For each BCD iteration: a. Solve two OT problems (one per outcome):<ul><li>G1 = argmin ⟨G, C1⟩ matching for Y prediction errors</li><li>G2 = argmin ⟨G, C2⟩ matching for Z prediction errors</li></ul>b. Transport outcomes using outcome-specific couplings:<ul><li>YB = nB·YA·G1 (Y transported for base B)</li><li>ZA = nA·ZB·G2&#39; (Z transported for base A)</li></ul>c. Train discriminant networks on transported outcomes:<ul><li>Train modelXYA on (XB, YB) for Y prediction</li><li>Train modelXZB on (XA, ZA) for Z prediction</li></ul>d. Compute prediction errors (cross-entropy):<ul><li>loss_y = YA vs modelXYA(XB)</li><li>loss_z = ZB vs modelXZB(XA)</li></ul>e. Update cost matrices with prediction error feedback:<ul><li>C1 ← C0 + loss_y</li><li>C2 ← C0 + loss_z&#39;</li></ul>f. Check convergence: transport plan stability or cost stability</li><li>Return argmax of final network predictions</li></ol><p><strong>Key Innovations of JDOT</strong></p><ul><li><strong>Separate couplings per outcome</strong>: G1 for Y matching, G2 for Z matching</li><li><strong>Outcome-specific cost</strong>: Each outcome has its own cost matrix driven by prediction errors</li><li><strong>Joint optimization</strong>: Balances covariate matching and outcome prediction accuracy</li><li><strong>Iterative refinement</strong>: Feedback loop where prediction errors guide next OT solve</li></ul><p><strong>Differences from Related Methods</strong></p><ul><li><strong>joint<em>ot</em>between<em>bases</em>with_predictors</strong>: Single transport plan G; symmetric outcome treatment</li><li><strong>joint<em>ot</em>between<em>bases</em>jdot</strong> (this): Two transport plans G1, G2; outcome-specific matching</li><li><strong>da<em>outcomes</em>with_predictors</strong>: Single static OT solve; no cost refinement</li><li><strong>joint<em>ot</em>between<em>bases</em>category</strong>: No discriminant networks; pure OT on outcomes</li></ul><p><strong>Details</strong></p><ul><li><strong>Separate OT problems</strong>: Each outcome solved independently, capturing outcome-specific distributions</li><li><strong>Cost matrix updates</strong>: Prediction errors guide next OT solve for better outcome alignment</li><li><strong>Cross-entropy loss</strong>: Weights outcome prediction errors by 1/n_levels for fair comparison</li><li><strong>Transport scaling</strong>: YB and ZA scaled by sample counts for proper probability aggregation</li><li><strong>Discriminant classifiers</strong>: Neural networks predict outcomes from covariates on transported samples</li><li><strong>Unbalanced OT</strong>: Uses KL divergence for regularization when reg &gt; 0</li></ul><p><strong>See Also</strong></p><ul><li><code>joint_ot_between_bases_with_predictors</code>: Single coupling variant</li><li><code>joint_ot_between_bases_da_outcomes_with_predictors</code>: DA variant (single static OT)</li><li><code>joint_ot_between_bases_category</code>: OT without discriminant networks</li><li><code>JointOTBetweenBases</code>: Main method dispatcher</li></ul><p><strong>Notes</strong></p><ul><li>Most sophisticated iterative approach: two independent transport problems per iteration</li><li>Computationally expensive: ~2× cost of single-coupling methods per iteration</li><li>Outcome-specific matching allows capturing different covariate-outcome relationships per outcome</li><li>Useful when Y and Z have fundamentally different relationships to X</li><li>Convergence typically faster than symmetric methods due to outcome-guided matching</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/otrecoding/OptimalTransportDataIntegration.jl/blob/972b019290e46df9e35cbe73eb02472dab094ed6/src/joint_ot_between_bases_jdot.jl#L1">source</a></section></details></article><article><details class="docstring" open="true"><summary id="OptimalTransportDataIntegration.joint_ot_between_bases_with_predictors-Tuple{Any}"><a class="docstring-binding" href="#OptimalTransportDataIntegration.joint_ot_between_bases_with_predictors-Tuple{Any}"><code>OptimalTransportDataIntegration.joint_ot_between_bases_with_predictors</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">joint_ot_between_bases_with_predictors(
    data;
    iterations,
    learning_rate,
    batchsize,
    epochs,
    hidden_layer_size,
    reg,
    reg_m1,
    reg_m2,
    Ylevels,
    Zlevels
)
</code></pre><p>Hybrid statistical matching combining optimal transport and neural network predictors.</p><p>Alternates between two operations: (1) solving optimal transport problem to match  base A and B samples by covariate-outcome combinations, and (2) training neural networks  to learn outcome prediction functions that minimize cross-entropy loss given transport plan. This block coordinate descent (BCD) algorithm jointly optimizes coupling and predictors.</p><p><strong>Arguments</strong></p><ul><li><code>data::DataFrame</code>: Input data with columns <code>database</code> (1 for base A, 2 for base B),  <code>X*</code> covariates, <code>Y</code> (outcome for base B), and <code>Z</code> (outcome for base A)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>iterations::Int</code>: Number of BCD iterations (OT + network training cycles); default: 10</li><li><code>learning_rate::Float64</code>: Adam optimizer learning rate for networks; default: 0.01</li><li><code>batchsize::Int</code>: Batch size for stochastic gradient descent; default: 512</li><li><code>epochs::Int</code>: Training epochs per network at each BCD iteration; default: 500</li><li><code>hidden_layer_size::Int</code>: Number of neurons in hidden layer; default: 10</li><li><code>reg::Float64</code>: Entropy regularization for OT (0 = exact, larger = relaxed); default: 0.0</li><li><code>reg_m1::Float64</code>: Marginal relaxation for base A; default: 0.0</li><li><code>reg_m2::Float64</code>: Marginal relaxation for base B; default: 0.0</li><li><code>Ylevels::AbstractRange</code>: Categorical levels for outcome Y; default: 1:4</li><li><code>Zlevels::AbstractRange</code>: Categorical levels for outcome Z; default: 1:3</li></ul><p><strong>Returns</strong></p><ul><li><code>Tuple{Vector{Int}, Vector{Int}}</code>: Predicted outcomes (YB, ZA)<ul><li><code>YB</code>: Final predictions for Y in base B (argmax of network outputs)</li><li><code>ZA</code>: Final predictions for Z in base A (argmax of network outputs)</li></ul></li></ul><p><strong>Algorithm (Block Coordinate Descent)</strong></p><ol><li>Initialize transport plan G, cost matrix C, and weights (uniform)</li><li>Concatenate covariates with outcomes: XYA = [XA; YA], XZB = [XB; ZB]</li><li>For each BCD iteration:<ul><li>Solve OT problem to compute coupling G minimizing C</li><li>Update outcome targets: YB = nB·YA·G, ZA = nA·ZB·G&#39;</li><li>Train predictor networks on updated targets</li><li>Compute cross-entropy loss and update cost matrix C ← C₀ + loss_feedback</li><li>Check convergence: transport plan stability (delta) or cost stability</li></ul></li><li>Return argmax of final network predictions</li></ol><p><strong>Details</strong></p><ul><li><strong>Transport plan</strong>: Couples samples across bases; G[i,j] represents mass transported from A[i] to B[j]</li><li><strong>Predictors</strong>: Neural networks learn mapping (X,outcome) → opposite_outcome with loss feedback</li><li><strong>Cost update</strong>: Cross-entropy loss between outcomes and network predictions guides next OT iteration</li><li><strong>Unbalanced OT</strong>: Uses KL divergence for regularization when reg &gt; 0</li></ul><p><strong>See Also</strong></p><ul><li><code>joint_ot_between_bases_category</code>: OT without neural network predictors</li><li><code>simple_learning</code>: Supervised baseline without OT coupling</li><li><code>JointOTBetweenBases</code>: Main method dispatcher</li></ul><p><strong>Notes</strong></p><ul><li>Computationally expensive: trains two networks at each of ~10 BCD iterations</li><li>Combines strength of OT (covariate matching) with flexibility of neural networks</li><li>Requires more iterations/epochs than pure OT for convergence</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/otrecoding/OptimalTransportDataIntegration.jl/blob/972b019290e46df9e35cbe73eb02472dab094ed6/src/joint_ot_between_bases_with_predictors.jl#L1">source</a></section></details></article><article><details class="docstring" open="true"><summary id="OptimalTransportDataIntegration.joint_ot_between_bases_without_outcomes-Tuple{Any}"><a class="docstring-binding" href="#OptimalTransportDataIntegration.joint_ot_between_bases_without_outcomes-Tuple{Any}"><code>OptimalTransportDataIntegration.joint_ot_between_bases_without_outcomes</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">joint_ot_between_bases_without_outcomes(
    data;
    iterations,
    learning_rate,
    batchsize,
    epochs,
    hidden_layer_size,
    reg,
    reg_m1,
    reg_m2,
    Ylevels,
    Zlevels
)
</code></pre><p>Optimal transport matching with outcome prediction from covariates only.</p><p>Solves OT problem on covariate space with single transport coupling G, then trains neural network predictors to map covariates to outcomes. Networks learn to predict missing outcomes directly from covariates (X → Y, X → Z) without using outcome combinations in the OT matching. This &quot;without  outcomes&quot; variant focuses on covariate-outcome mapping rather than joint distribution transport.</p><p><strong>Arguments</strong></p><ul><li><code>data::DataFrame</code>: Input data with columns <code>database</code> (1 for base A, 2 for base B),  <code>X*</code> covariates, <code>Y</code> (outcome for base B), and <code>Z</code> (outcome for base A)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>iterations::Int</code>: Number of BCD iterations (OT + network training cycles); default: 10</li><li><code>learning_rate::Float64</code>: Adam optimizer learning rate for networks; default: 0.01</li><li><code>batchsize::Int</code>: Batch size for stochastic gradient descent; default: 512</li><li><code>epochs::Int</code>: Training epochs per network at each BCD iteration; default: 1000</li><li><code>hidden_layer_size::Int</code>: Number of neurons in hidden layer; default: 10</li><li><code>reg::Float64</code>: Entropy regularization for OT (0 = exact, larger = relaxed); default: 0.0</li><li><code>reg_m1::Float64</code>: Marginal relaxation for base A; default: 0.0</li><li><code>reg_m2::Float64</code>: Marginal relaxation for base B; default: 0.0</li><li><code>Ylevels::AbstractRange</code>: Categorical levels for outcome Y; default: 1:4</li><li><code>Zlevels::AbstractRange</code>: Categorical levels for outcome Z; default: 1:3</li></ul><p><strong>Returns</strong></p><ul><li><code>Tuple{Vector{Int}, Vector{Int}}</code>: Predicted outcomes (YB, ZA)<ul><li><code>YB</code>: Final predictions for Y in base B (argmax of network outputs)</li><li><code>ZA</code>: Final predictions for Z in base A (argmax of network outputs)</li></ul></li></ul><p><strong>Algorithm (Block Coordinate Descent)</strong></p><ol><li>Initialize single transport plan G and covariate-only predictor networks</li><li>For each BCD iteration: a. Solve OT problem on covariate space only:<ul><li>G = argmin ⟨G, C⟩ (no outcome information in cost)</li></ul>b. Transport outcome distributions using single coupling:<ul><li>YB = nB·YA·G (Y transported for base B)</li><li>ZA = nA·ZB·G&#39; (Z transported for base A)</li></ul>c. Train predictor networks from covariates to transported outcomes:<ul><li>Train modelXA on (XA, ZA) - learn X → Z</li><li>Train modelXB on (XB, YB) - learn X → Y</li></ul>d. Compute prediction errors (cross-entropy):<ul><li>loss_y = YA vs modelXB(XB)</li><li>loss_z = ZB vs modelXA(XA)</li></ul>e. Update cost matrix with prediction error feedback:<ul><li>C ← C₀ + loss<em>y + loss</em>z&#39;</li></ul>f. Check convergence: transport plan stability or cost stability</li><li>Return argmax of final network predictions</li></ol><p><strong>Key Differences from Similar Methods</strong></p><ul><li><strong>without_outcomes</strong> (this): OT on X only; networks predict X → Y, X → Z</li><li><strong>with_predictors</strong>: OT on X; networks train on transported (X,outcome) pairs</li><li><strong>jdot</strong>: Two separate OT problems (one per outcome); outcome-specific matching</li><li><strong>da_outcomes</strong>: OT on X; direct outcome distribution transport (no networks)</li></ul><p><strong>Details</strong></p><ul><li><strong>Covariate-only OT</strong>: Cost matrix depends only on X distance, ignoring outcomes</li><li><strong>Symmetric treatment</strong>: Both Y and Z use same transport coupling G</li><li><strong>Loss-driven refinement</strong>: Prediction errors guide next OT cost matrix</li><li><strong>Network inputs</strong>: Covariate-only (X), not joint (X,outcome)</li><li><strong>Transport targets</strong>: Networks learn to match transported outcome distributions</li><li><strong>Unbalanced OT</strong>: Uses KL divergence for regularization when reg &gt; 0</li><li><strong>Single coupling</strong>: Unlike JDOT (2 couplings), uses one G for both outcomes</li></ul><p><strong>See Also</strong></p><ul><li><code>joint_ot_between_bases_with_predictors</code>: Joint (X,outcome) OT with predictors</li><li><code>joint_ot_between_bases_jdot</code>: Two separate OT problems (outcome-specific)</li><li><code>joint_ot_between_bases_da_covariables</code>: DA on covariates (no outcome info)</li><li><code>JointOTBetweenBases</code>: Main method dispatcher</li></ul><p><strong>Notes</strong></p><ul><li>Simplest BCD variant: matches on covariates, learns outcome relationships via networks</li><li>Computationally efficient: single OT solve per iteration (vs 2 for JDOT)</li><li>Outcome-agnostic matching: OT doesn&#39;t see outcome relationships directly</li><li>Two networks learn identical relationship (X → Y and X → Z) from different transported targets</li><li>Useful when outcome dimensions are complex or when covariate alignment is priority</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/otrecoding/OptimalTransportDataIntegration.jl/blob/972b019290e46df9e35cbe73eb02472dab094ed6/src/joint_ot_between_bases_without_outcomes.jl#L1">source</a></section></details></article><article><details class="docstring" open="true"><summary id="OptimalTransportDataIntegration.joint_ot_between_bases_category-NTuple{4, Any}"><a class="docstring-binding" href="#OptimalTransportDataIntegration.joint_ot_between_bases_category-NTuple{4, Any}"><code>OptimalTransportDataIntegration.joint_ot_between_bases_category</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">joint_ot_between_bases_category(
    data,
    reg,
    reg_m1,
    reg_m2;
    Ylevels,
    Zlevels,
    iterations
)
</code></pre><p>Statistical matching via optimal transport between two bases with categorical outcomes.</p><p>This function integrates two data sources (base A and base B) using optimal transport theory to match and predict missing categorical outcomes (Y and Z). It transports joint distributions of covariates and outcomes across bases while iteratively minimizing cross-entropy loss for outcome predictions.</p><p><strong>Arguments</strong></p><ul><li><code>data::DataFrame</code>: Input data with columns <code>database</code> (1 for base A, 2 for base B),  <code>X*</code> covariates, <code>Y</code> (outcome for base B), and <code>Z</code> (outcome for base A)</li><li><code>reg::Float64</code>: Entropy regularization parameter for OT solver (0 = exact OT, larger = more relaxed)</li><li><code>reg_m1::Float64</code>: Marginal constraint relaxation for base A (set to 0 for balanced problem)</li><li><code>reg_m2::Float64</code>: Marginal constraint relaxation for base B (set to 0 for balanced problem)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>Ylevels::AbstractRange</code>: Categorical levels for outcome Y; default: 1:4</li><li><code>Zlevels::AbstractRange</code>: Categorical levels for outcome Z; default: 1:3</li><li><code>iterations::Int</code>: Number of algorithm iterations for cost matrix refinement; default: 1</li></ul><p><strong>Returns</strong></p><ul><li><code>Tuple{Vector{Int32}, Vector{Int32}}</code>: Predicted outcomes (YB<em>pred, ZA</em>pred)<ul><li><code>YB_pred</code>: Predictions for Y in base A (same length as base A observations)</li><li><code>ZA_pred</code>: Predictions for Z in base B (same length as base B observations)</li></ul></li></ul><p><strong>Algorithm</strong></p><ol><li>Aggregate individuals by covariate and outcome combinations</li><li>Initialize transport plan G and cost matrix C based on Euclidean distance</li><li>Iterate:<ul><li>Solve OT problem (unbalanced or exact) via PythonOT</li><li>Predict outcomes by minimizing cross-entropy loss per transport cost</li><li>Update cost matrix with prediction error feedback</li></ul></li><li>Return predicted outcomes mapped back to original base structure</li></ol><p><strong>Details</strong></p><ul><li>One-hot encodes categorical outcomes for cross-entropy loss computation</li><li>Uses unbalanced OT (KL divergence) when reg<em>m1 &gt; 0 and reg</em>m2 &gt; 0, else exact EMD</li><li>Convergence: detects transport plan convergence (delta &lt; 1e-16) or cost stability (&lt; 1e-7)</li></ul><p><strong>See Also</strong></p><ul><li><code>joint_ot_between_bases_discrete</code>: Discrete covariates version</li><li><code>one_hot_encoder</code>: Converts categorical data to one-hot matrix form</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/otrecoding/OptimalTransportDataIntegration.jl/blob/972b019290e46df9e35cbe73eb02472dab094ed6/src/joint_ot_between_bases_category.jl#L7">source</a></section></details></article><article><details class="docstring" open="true"><summary id="OptimalTransportDataIntegration.joint_ot_between_bases_da_covariables-Tuple{Any}"><a class="docstring-binding" href="#OptimalTransportDataIntegration.joint_ot_between_bases_da_covariables-Tuple{Any}"><code>OptimalTransportDataIntegration.joint_ot_between_bases_da_covariables</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">joint_ot_between_bases_da_covariables(
    data;
    learning_rate,
    batchsize,
    epochs,
    hidden_layer_size,
    reg,
    reg_m1,
    reg_m2,
    Ylevels,
    Zlevels
)
</code></pre><p>Optimal transport matching with discriminant analysis on covariates.</p><p>Combines optimal transport on covariate space with neural network discriminant classifiers. Solves OT problem based on covariate-only distance, then trains neural networks to predict outcomes from transported covariates. The &quot;DA&quot; (Discriminant Analysis) prefix indicates that prediction uses only covariates (X), not the joint (X,outcome) space like other DA variants.</p><p><strong>Arguments</strong></p><ul><li><code>data::DataFrame</code>: Input data with columns <code>database</code> (1 for base A, 2 for base B),  <code>X*</code> covariates, <code>Y</code> (outcome for base B), and <code>Z</code> (outcome for base A)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>learning_rate::Float64</code>: Adam optimizer learning rate for networks; default: 0.01</li><li><code>batchsize::Int</code>: Batch size for stochastic gradient descent; default: 512</li><li><code>epochs::Int</code>: Training epochs per network; default: 500</li><li><code>hidden_layer_size::Int</code>: Number of neurons in hidden layer; default: 10</li><li><code>reg::Float64</code>: Entropy regularization for OT (0 = exact, larger = relaxed); default: 0.0</li><li><code>reg_m1::Float64</code>: Marginal relaxation for base A; default: 0.0</li><li><code>reg_m2::Float64</code>: Marginal relaxation for base B; default: 0.0</li><li><code>Ylevels::AbstractRange</code>: Categorical levels for outcome Y; default: 1:4</li><li><code>Zlevels::AbstractRange</code>: Categorical levels for outcome Z; default: 1:3</li></ul><p><strong>Returns</strong></p><ul><li><code>Tuple{Vector{Int}, Vector{Int}}</code>: Predicted outcomes (YB, ZA)<ul><li><code>YB</code>: Final predictions for Y in base B (argmax of network outputs)</li><li><code>ZA</code>: Final predictions for Z in base A (argmax of network outputs)</li></ul></li></ul><p><strong>Algorithm</strong></p><ol><li>Initialize networks for outcome prediction: modelXYA (X → Y), modelXZB (X → Z)</li><li>Compute cost matrix C based on Euclidean distance in <strong>covariate space only</strong> (not outcomes)</li><li>Solve OT problem: find coupling G that minimizes ⟨G, C⟩</li><li>Transport covariates: XA<em>transported = nB·XA·G, XB</em>transported = nA·XB·G&#39;</li><li>Train networks on transported covariates:<ul><li>Train modelXYA on (XB_transported, YA)</li><li>Train modelXZB on (XA_transported, ZB)</li></ul></li><li>Return final predictions on original covariates</li></ol><p><strong>Key Differences from Other DA Variants</strong></p><ul><li><strong>da_covariables</strong> (this): OT on X only; predict Y,Z from X (covariate-only discriminant analysis)</li><li><strong>da_outcomes</strong>: OT on joint (X,Y) and (X,Z); may refine based on outcome prediction errors</li><li><strong>da<em>outcomes</em>with_predictors</strong>: Similar to da_outcomes but with explicit predictor networks</li></ul><p><strong>Details</strong></p><ul><li><strong>Cost matrix</strong>: Based purely on covariate distances (SqEuclidean)</li><li><strong>Network training</strong>: Uses transported covariates as targets to match covariate distributions</li><li><strong>Transport plan</strong>: Couples full samples (not disaggregated by outcome), only balances marginal covariate distributions</li><li><strong>Unbalanced OT</strong>: Uses KL divergence for regularization when reg &gt; 0</li></ul><p><strong>See Also</strong></p><ul><li><code>joint_ot_between_bases_da_outcomes</code>: OT on outcomes with discriminant prediction</li><li><code>joint_ot_between_bases_da_outcomes_with_predictors</code>: Hybrid with explicit predictor networks</li><li><code>joint_ot_between_bases_category</code>: OT without discriminant analysis</li><li><code>joint_ot_between_bases_with_predictors</code>: OT with flexible predictor networks</li></ul><p><strong>Notes</strong></p><ul><li>Computationally less expensive than outcome-based DA variants</li><li>May lose information by ignoring outcome distributions in OT matching</li><li>Network training uses transported covariate targets, not standard supervised learning</li><li>Useful when covariate matching is priority and outcome prediction is secondary</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/otrecoding/OptimalTransportDataIntegration.jl/blob/972b019290e46df9e35cbe73eb02472dab094ed6/src/joint_ot_between_bases_da_covariables.jl#L1">source</a></section></details></article><article><details class="docstring" open="true"><summary id="OptimalTransportDataIntegration.joint_ot_between_bases_da_outcomes-Tuple{Any}"><a class="docstring-binding" href="#OptimalTransportDataIntegration.joint_ot_between_bases_da_outcomes-Tuple{Any}"><code>OptimalTransportDataIntegration.joint_ot_between_bases_da_outcomes</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">joint_ot_between_bases_da_outcomes(
    data;
    iterations,
    learning_rate,
    batchsize,
    epochs,
    hidden_layer_size,
    reg,
    reg_m1,
    reg_m2,
    Ylevels,
    Zlevels
)
</code></pre><p>Discriminant analysis via optimal transport on covariate-outcome space.</p><p>Solves a single optimal transport problem to match distributions across bases, then uses the resulting transport coupling to predict outcomes. Unlike iterative refinement methods, this approach computes outcomes directly from transported outcome distributions without predictor networks. The &quot;DA outcomes&quot; variant focuses on transporting outcome probability distributions aligned with covariate matching.</p><p><strong>Arguments</strong></p><ul><li><code>data::DataFrame</code>: Input data with columns <code>database</code> (1 for base A, 2 for base B),  <code>X*</code> covariates, <code>Y</code> (outcome for base B), and <code>Z</code> (outcome for base A)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>iterations::Int</code>: Unused parameter (kept for API compatibility); default: 10</li><li><code>learning_rate::Float64</code>: Unused parameter (kept for API compatibility); default: 0.01</li><li><code>batchsize::Int</code>: Unused parameter (kept for API compatibility); default: 512</li><li><code>epochs::Int</code>: Unused parameter (kept for API compatibility); default: 500</li><li><code>hidden_layer_size::Int</code>: Unused parameter (kept for API compatibility); default: 10</li><li><code>reg::Float64</code>: Entropy regularization for OT (0 = exact, larger = relaxed); default: 0.0</li><li><code>reg_m1::Float64</code>: Marginal relaxation for base A; default: 0.0</li><li><code>reg_m2::Float64</code>: Marginal relaxation for base B; default: 0.0</li><li><code>Ylevels::AbstractRange</code>: Categorical levels for outcome Y; default: 1:4</li><li><code>Zlevels::AbstractRange</code>: Categorical levels for outcome Z; default: 1:3</li></ul><p><strong>Returns</strong></p><ul><li><code>Tuple{Vector{Int}, Vector{Int}}</code>: Predicted outcomes (YB, ZA)<ul><li><code>YB</code>: Final predictions for Y in base B (argmax of transported outcome probabilities)</li><li><code>ZA</code>: Final predictions for Z in base A (argmax of transported outcome probabilities)</li></ul></li></ul><p><strong>Algorithm</strong></p><ol><li>Compute cost matrix C based on Euclidean distance between covariates (X only)</li><li>Initialize uniform weights: wa = 1/nA, wb = 1/nB</li><li>Solve OT problem once: G = argmin ⟨G, C⟩ subject to marginal constraints<ul><li>Uses unbalanced OT (KL divergence) if reg &gt; 0, else exact EMD</li></ul></li><li>Transport outcome distributions using coupling G:<ul><li>YBpred = softmax(nB·YA·G): transported Y probabilities for base B samples</li><li>ZApred = softmax(nA·ZB·G&#39;): transported Z probabilities for base A samples</li></ul></li><li>Return argmax of transported probabilities as final predictions</li></ol><p><strong>Key Differences from Other DA Variants</strong></p><ul><li><strong>da_covariables</strong>: OT on X only; predicts outcomes from transported covariates via networks</li><li><strong>da_outcomes</strong> (this): OT on X; predicts outcomes directly from transported outcome distributions</li><li><strong>da<em>outcomes</em>with_predictors</strong>: OT on X; trains networks on transported outcome targets</li><li><strong>joint<em>ot</em>between<em>bases</em>with_predictors</strong>: Iterative BCD with loss-driven cost updates</li></ul><p><strong>Details</strong></p><ul><li><strong>Cost matrix</strong>: Based purely on covariate distances (SqEuclidean), not normalized</li><li><strong>Single OT solve</strong>: Static cost matrix, solved once (no iterative refinement)</li><li><strong>Outcome transport</strong>: Coupling G transports outcome probability distributions across bases</li><li><strong>Direct prediction</strong>: Predictions come directly from transported probabilities (softmax + argmax)</li><li><strong>No networks</strong>: Unlike with_predictors variant, uses no neural networks</li><li><strong>Unbalanced OT</strong>: Uses KL divergence for regularization when reg &gt; 0</li><li><strong>Scaling factor</strong>: Transportation weighted by sample count (nB, nA) for proper probability aggregation</li></ul><p><strong>See Also</strong></p><ul><li><code>joint_ot_between_bases_da_outcomes_with_predictors</code>: DA outcomes with explicit predictor networks</li><li><code>joint_ot_between_bases_da_covariables</code>: DA on covariates with covariate-based prediction</li><li><code>joint_ot_between_bases_category</code>: OT without discriminant analysis</li><li><code>joint_ot_between_bases_with_predictors</code>: Iterative BCD variant with cost refinement</li></ul><p><strong>Notes</strong></p><ul><li>Simplest DA variant: combines OT matching with direct outcome transport</li><li>Most computationally efficient: single OT solve, no network training</li><li>Outcome predictions are soft probabilities from transported distributions</li><li>Useful for understanding how outcome distributions align under OT matching</li><li>No learnable parameters; purely distribution-based prediction</li><li>Hyperparameters for networks (learning_rate, etc.) are ignored for API compatibility</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/otrecoding/OptimalTransportDataIntegration.jl/blob/972b019290e46df9e35cbe73eb02472dab094ed6/src/joint_ot_between_bases_da_outcomes.jl#L1">source</a></section></details></article><article><details class="docstring" open="true"><summary id="OptimalTransportDataIntegration.joint_ot_between_bases_da_outcomes_with_predictors-Tuple{Any}"><a class="docstring-binding" href="#OptimalTransportDataIntegration.joint_ot_between_bases_da_outcomes_with_predictors-Tuple{Any}"><code>OptimalTransportDataIntegration.joint_ot_between_bases_da_outcomes_with_predictors</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">joint_ot_between_bases_da_outcomes_with_predictors(
    data;
    iterations,
    learning_rate,
    batchsize,
    epochs,
    hidden_layer_size,
    reg,
    reg_m1,
    reg_m2,
    Ylevels,
    Zlevels
)
</code></pre><p>Advanced discriminant analysis: OT on covariate space with outcome-based predictor refinement.</p><p>Solves a single optimal transport problem on covariate-only distance, then trains neural networks to predict outcomes from transported samples. This &quot;DA with predictors&quot; variant uses the OT coupling to transport outcome distributions and trains networks on transported outcome targets (not iterative). Combines covariate-based matching with outcome prediction flexibility.</p><p><strong>Arguments</strong></p><ul><li><code>data::DataFrame</code>: Input data with columns <code>database</code> (1 for base A, 2 for base B),  <code>X*</code> covariates, <code>Y</code> (outcome for base B), and <code>Z</code> (outcome for base A)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>iterations::Int</code>: Unused parameter (kept for API compatibility); default: 10</li><li><code>learning_rate::Float64</code>: Adam optimizer learning rate for networks; default: 0.01</li><li><code>batchsize::Int</code>: Batch size for stochastic gradient descent; default: 512</li><li><code>epochs::Int</code>: Training epochs per network; default: 500</li><li><code>hidden_layer_size::Int</code>: Number of neurons in hidden layer; default: 10</li><li><code>reg::Float64</code>: Entropy regularization for OT (0 = exact, larger = relaxed); default: 0.0</li><li><code>reg_m1::Float64</code>: Marginal relaxation for base A; default: 0.0</li><li><code>reg_m2::Float64</code>: Marginal relaxation for base B; default: 0.0</li><li><code>Ylevels::AbstractRange</code>: Categorical levels for outcome Y; default: 1:4</li><li><code>Zlevels::AbstractRange</code>: Categorical levels for outcome Z; default: 1:3</li></ul><p><strong>Returns</strong></p><ul><li><code>Tuple{Vector{Int}, Vector{Int}}</code>: Predicted outcomes (YB, ZA)<ul><li><code>YB</code>: Final predictions for Y in base B (argmax of network outputs)</li><li><code>ZA</code>: Final predictions for Z in base A (argmax of network outputs)</li></ul></li></ul><p><strong>Algorithm</strong></p><ol><li>Initialize outcome prediction networks: modelXYA (X → Y), modelXZB (X → Z)</li><li>Compute cost matrix C based on Euclidean distance in <strong>covariate space only</strong></li><li>Solve OT problem once: G = argmin ⟨G, C⟩ subject to marginal constraints</li><li>Transport outcome distributions (not covariates):<ul><li>YBt = nB·YA·G (transported Y probabilities for base B samples)</li><li>ZAt = nA·ZB·G&#39; (transported Z probabilities for base A samples)</li></ul></li><li>Train networks on transported outcome targets:<ul><li>Train modelXYA on (XB, YBt) - predict transported Y from base B covariates</li><li>Train modelXZB on (XA, ZAt) - predict transported Z from base A covariates</li></ul></li><li>Return final predictions on original covariates</li></ol><p><strong>Key Differences from Other DA Variants</strong></p><ul><li><strong>da_covariables</strong>: OT on X only; networks train on transported covariates (not outcomes)</li><li><strong>da_outcomes</strong>: OT on joint (X,Y) and (X,Z); iterative refinement (not implemented here)</li><li><strong>da<em>outcomes</em>with_predictors</strong> (this): OT on X only; networks train on transported <strong>outcomes</strong></li><li><strong>joint<em>ot</em>between<em>bases</em>with_predictors</strong>: Iterative BCD with loss-driven cost updates</li></ul><p><strong>Details</strong></p><ul><li><strong>Cost matrix</strong>: Based purely on covariate distances (SqEuclidean), normalized</li><li><strong>Single OT solve</strong>: Unlike BCD methods, OT problem solved once with static cost</li><li><strong>Outcome transport</strong>: Coupling G transports outcome probability distributions across bases</li><li><strong>Network targets</strong>: Soft probabilities (one-hot encoded) from transported distributions</li><li><strong>Unbalanced OT</strong>: Uses KL divergence for regularization when reg &gt; 0</li><li><strong>Prediction</strong>: Uses softmax probabilities, then argmax for final discrete predictions</li></ul><p><strong>See Also</strong></p><ul><li><code>joint_ot_between_bases_da_outcomes</code>: Similar approach (outcomes) without explicit predictors</li><li><code>joint_ot_between_bases_da_covariables</code>: OT on covariates with covariate-based DA</li><li><code>joint_ot_between_bases_with_predictors</code>: Iterative BCD variant with cost refinement</li><li><code>joint_ot_between_bases_category</code>: OT without discriminant analysis component</li></ul><p><strong>Notes</strong></p><ul><li>More computationally efficient than iterative BCD methods (single OT solve)</li><li>Blends OT matching (for covariate alignment) with neural network flexibility (outcome prediction)</li><li>Network training targets are transported outcome distributions, not standard supervised learning</li><li>Outcome transport via coupling ensures covariate-aligned outcome distributions</li><li>Useful when outcome prediction needs flexibility beyond linear transport assumptions</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/otrecoding/OptimalTransportDataIntegration.jl/blob/972b019290e46df9e35cbe73eb02472dab094ed6/src/joint_ot_between_bases_da_outcomes_with_predictors.jl#L1">source</a></section></details></article><article><details class="docstring" open="true"><summary id="OptimalTransportDataIntegration.joint_ot_between_bases_discrete-NTuple{4, Any}"><a class="docstring-binding" href="#OptimalTransportDataIntegration.joint_ot_between_bases_discrete-NTuple{4, Any}"><code>OptimalTransportDataIntegration.joint_ot_between_bases_discrete</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">joint_ot_between_bases_discrete(
    data,
    reg,
    reg_m1,
    reg_m2;
    Ylevels,
    Zlevels,
    iterations,
    distance
)
</code></pre><p>Statistical matching via optimal transport for discrete (categorical) covariates.</p><p>Specialized implementation for discrete covariate data. Aggregates individuals by unique covariate-outcome combinations to reduce computational burden. Uses one-hot encoded covariates and solves OT problems on the aggregated space. Iteratively minimizes cross-entropy loss between predicted and transported outcomes to guide cost matrix refinement.</p><p><strong>Arguments</strong></p><ul><li><code>data::DataFrame</code>: Input data with columns <code>database</code> (1 for base A, 2 for base B),  <code>X*</code> covariates (must be categorical/integer), <code>Y</code> (outcome for base B), and <code>Z</code> (outcome for base A)</li><li><code>reg::Float64</code>: Entropy regularization parameter for OT solver</li><li><code>reg_m1::Float64</code>: Marginal constraint relaxation for base A</li><li><code>reg_m2::Float64</code>: Marginal constraint relaxation for base B</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>Ylevels::AbstractRange</code>: Categorical levels for outcome Y; default: 1:4</li><li><code>Zlevels::AbstractRange</code>: Categorical levels for outcome Z; default: 1:3</li><li><code>iterations::Int</code>: Number of algorithm iterations; default: 1</li><li><code>distance::Distances.Metric</code>: Distance metric for covariate space; default: Euclidean()</li></ul><p><strong>Returns</strong></p><ul><li><code>Tuple{Vector{Int}, Vector{Int}}</code>: Predicted outcomes (YB, ZA)<ul><li><code>YB</code>: Predictions for Y in base B</li><li><code>ZA</code>: Predictions for Z in base A</li></ul></li></ul><p><strong>Algorithm</strong></p><ol><li>One-hot encode discrete covariates</li><li>Aggregate individuals by unique (X, outcome) combinations:<ul><li>Reduces individuals to &quot;cells&quot; (covariate × outcome modality combinations)</li><li>Computes cell weights as proportions of total sample size</li><li>Filters out empty cells</li></ul></li><li>Compute pairwise distances between covariate profiles in aggregated space</li><li>Initialize outcome predictions and loss matrices</li><li>For each iteration:<ul><li>Solve OT problem on aggregated cells with current cost matrix</li><li>Predict outcomes by minimizing cross-entropy loss (argmin over modalities)</li><li>Compute prediction error (cross-entropy between transported and predicted outcomes)</li><li>Update cost matrix with loss feedback</li><li>Check convergence</li></ul></li><li>Map predictions back to original individual-level data</li><li>Return individual-level outcome predictions</li></ol><p><strong>Computational Efficiency Tricks</strong></p><ul><li><strong>Aggregation</strong>: Reduces from n individuals to ~(n<em>covariates × n</em>outcomes) cells<ul><li>E.g., 1000 individuals with 3 covariates (2,3,4 levels) + 2 outcomes → ~200-300 cells</li></ul></li><li><strong>One-hot encoding</strong>: Discrete covariates represented as binary vectors for distance computation</li><li><strong>Filtered weights</strong>: Only processes cells with positive weight (observed data combinations)</li><li><strong>Instance pre-computation</strong>: Distance matrix computed once between all covariate profiles</li></ul><p><strong>Details</strong></p><ul><li><strong>Data aggregation</strong>: Crucial for computational tractability with discrete data</li><li><strong>Distance metric</strong>: Determines covariate similarity (Euclidean on one-hot vectors by default)</li><li><strong>Cost matrix</strong>: Initial distance-based, updated with loss feedback</li><li><strong>Cross-entropy</strong>: Compares one-hot encoded outcomes for alignment</li><li><strong>Convergence</strong>: Checks transport plan stability (delta) and cost stability</li></ul><p><strong>See Also</strong></p><ul><li><code>joint_ot_between_bases_category</code>: Continuous covariate version with categorical outcomes</li><li><code>Instance</code>: Pre-computed distance structure used in aggregation</li></ul><p><strong>Notes</strong></p><ul><li><strong>Discrete specialization</strong>: Exploits discrete structure via aggregation for efficiency</li><li><strong>Cell-level matching</strong>: Matches entire cells (all individuals with same X and outcome)</li><li><strong>Cross-entropy loss</strong>: Drives iterative cost refinement for outcome prediction accuracy</li><li><strong>Outcome aggregation</strong>: Predictions map back from cell-level to individual-level deterministically</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/otrecoding/OptimalTransportDataIntegration.jl/blob/972b019290e46df9e35cbe73eb02472dab094ed6/src/joint_ot_between_bases_discrete.jl#L9">source</a></section></details></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../joint_ot_within_base/">« Transport within a data source</a><a class="docs-footer-nextpage" href="../learning/">Machine Learning »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Thursday 8 January 2026 14:30">Thursday 8 January 2026</span>. Using Julia version 1.12.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
